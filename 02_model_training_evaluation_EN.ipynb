{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Model Training and Evaluation\n",
    "This notebook contains training and evaluation of machine learning models for regression.\n",
    "\n",
    "## Models to evaluate:\n",
    "1. **Multivariate Linear Regression** (LinearRegression)\n",
    "2. **Support Vector Regression (SVR)**\n",
    "3. **XGBoost Regressor**\n",
    "\n",
    "## Objectives:\n",
    "- Train models with different hyperparameters\n",
    "- Evaluate performance with regression metrics\n",
    "- Analyze feature importance\n",
    "- Compare models and select the best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, validation_curve\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "#import xgboost as xgb\n",
    "from scipy import stats\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../cloud-data/its-cmo-darwin-magellan-workspaces-folders/WS_PMCB/BisCiT_Repository/results/current_version/v2.0c/aa/dtl-surfaceome-secretome-equal_0.1/results-with-evidence-full.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully: 11943461 rows and 19 columns\n"
     ]
    }
   ],
   "source": [
    "file_path = dataset_path\n",
    "\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "print(f\"Dataset loaded successfully: {df.shape[0]} rows and {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gene 1</th>\n",
       "      <th>Gene 2</th>\n",
       "      <th>AA Score</th>\n",
       "      <th>Genetics</th>\n",
       "      <th>Knowledge Graph Connectivity</th>\n",
       "      <th>Single Cell DE</th>\n",
       "      <th>Single Cell Expression</th>\n",
       "      <th>Single Cell Specificity</th>\n",
       "      <th>Ligand Receptor Significance</th>\n",
       "      <th>SC KO, positive</th>\n",
       "      <th>Credentialing</th>\n",
       "      <th>Credentialing bulk RNA</th>\n",
       "      <th>SC KO, Negative</th>\n",
       "      <th>DTL, Th1-17</th>\n",
       "      <th>DTL, Th2</th>\n",
       "      <th>Immune Competition</th>\n",
       "      <th>Single Cell Co-dysregulation, positive</th>\n",
       "      <th>Single Cell Co-dysregulation, negative</th>\n",
       "      <th>Single Cell Co-dysregulation, mixed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CTLA4</td>\n",
       "      <td>IL2RA</td>\n",
       "      <td>3.750244</td>\n",
       "      <td>0.850673</td>\n",
       "      <td>0.932113</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.266357</td>\n",
       "      <td>0.230324</td>\n",
       "      <td>0.363851</td>\n",
       "      <td>0.157781</td>\n",
       "      <td>0.952958</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.003812</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IL2RA</td>\n",
       "      <td>TNFRSF1A</td>\n",
       "      <td>3.516827</td>\n",
       "      <td>0.723262</td>\n",
       "      <td>0.741249</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.334782</td>\n",
       "      <td>0.177688</td>\n",
       "      <td>0.462156</td>\n",
       "      <td>0.480434</td>\n",
       "      <td>0.669561</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.072305</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Gene 1    Gene 2  AA Score  Genetics  Knowledge Graph Connectivity  \\\n",
       "0  CTLA4     IL2RA  3.750244  0.850673                      0.932113   \n",
       "1  IL2RA  TNFRSF1A  3.516827  0.723262                      0.741249   \n",
       "\n",
       "   Single Cell DE  Single Cell Expression  Single Cell Specificity  \\\n",
       "0             0.0                0.266357                 0.230324   \n",
       "1             0.0                0.334782                 0.177688   \n",
       "\n",
       "   Ligand Receptor Significance  SC KO, positive  Credentialing  \\\n",
       "0                      0.363851         0.157781       0.952958   \n",
       "1                      0.462156         0.480434       0.669561   \n",
       "\n",
       "   Credentialing bulk RNA  SC KO, Negative  DTL, Th1-17  DTL, Th2  \\\n",
       "0                     0.0        -0.003812          NaN       NaN   \n",
       "1                     0.0        -0.072305          NaN       NaN   \n",
       "\n",
       "   Immune Competition  Single Cell Co-dysregulation, positive  \\\n",
       "0                 0.0                                     0.0   \n",
       "1                 0.0                                     0.0   \n",
       "\n",
       "   Single Cell Co-dysregulation, negative  Single Cell Co-dysregulation, mixed  \n",
       "0                                     0.0                                  0.0  \n",
       "1                                     0.0                                  0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Gene 1', 'Gene 2', 'AA Score', 'Genetics',\n",
       "       'Knowledge Graph Connectivity', 'Single Cell DE',\n",
       "       'Single Cell Expression', 'Single Cell Specificity',\n",
       "       'Ligand Receptor Significance', 'SC KO, positive', 'Credentialing',\n",
       "       'Credentialing bulk RNA', 'SC KO, Negative', 'DTL, Th1-17', 'DTL, Th2',\n",
       "       'Immune Competition', 'Single Cell Co-dysregulation, positive',\n",
       "       'Single Cell Co-dysregulation, negative',\n",
       "       'Single Cell Co-dysregulation, mixed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection of features and target columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_columns = ['Genetics',\n",
    "       'Knowledge Graph Connectivity', 'Single Cell DE',\n",
    "       'Single Cell Expression', 'Single Cell Specificity',\n",
    "       'Ligand Receptor Significance', 'SC KO, positive', 'Credentialing',\n",
    "       'Credentialing bulk RNA', 'SC KO, Negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['Genetics', 'Knowledge Graph Connectivity', 'Single Cell DE', 'Single Cell Expression', 'Single Cell Specificity', 'Ligand Receptor Significance', 'SC KO, positive', 'Credentialing', 'Credentialing bulk RNA', 'SC KO, Negative']\n",
      "X shape: (11943461, 10)\n",
      "y shape: (11943461,)\n",
      "\n",
      "Missing values in X: 0\n",
      "Missing values in y: 0\n"
     ]
    }
   ],
   "source": [
    "# Data preparation\n",
    "# Separate features and target variable\n",
    "feature_cols = [col for col in features_columns]\n",
    "X = df[feature_cols]\n",
    "y = df['AA Score']\n",
    "\n",
    "print(f\"Selected features: {feature_cols}\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_X = X.isnull().sum().sum()\n",
    "missing_y = y.isnull().sum()\n",
    "print(f\"\\nMissing values in X: {missing_X}\")\n",
    "print(f\"Missing values in y: {missing_y}\")\n",
    "\n",
    "# Remove rows with missing values if they exist\n",
    "if missing_X > 0 or missing_y > 0:\n",
    "    mask = ~(X.isnull().any(axis=1) | y.isnull())\n",
    "    X = X[mask]\n",
    "    y = y[mask]\n",
    "    print(f\"After cleaning data: X={X.shape}, y={y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 9554768 samples\n",
      "Test set: 2388693 samples\n",
      "Train/test ratio: 4.0\n",
      "\n",
      "Target variable statistics:\n",
      "Train - Mean: 0.22, Std: 0.24\n",
      "Test  - Mean: 0.22, Std: 0.24\n"
     ]
    }
   ],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=None\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Train/test ratio: {X_train.shape[0]/X_test.shape[0]:.1f}\")\n",
    "\n",
    "# Target variable statistics\n",
    "print(f\"\\nTarget variable statistics:\")\n",
    "print(f\"Train - Mean: {y_train.mean():.2f}, Std: {y_train.std():.2f}\")\n",
    "print(f\"Test  - Mean: {y_test.mean():.2f}, Std: {y_test.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalers prepared:\n",
      "  ‚Ä¢ StandardScaler\n",
      "  ‚Ä¢ RobustScaler\n",
      "  ‚Ä¢ NoScaler\n"
     ]
    }
   ],
   "source": [
    "# Prepare different scalers\n",
    "scalers = {\n",
    "    'StandardScaler': StandardScaler(),\n",
    "    'RobustScaler': RobustScaler(),\n",
    "    'NoScaler': None\n",
    "}\n",
    "\n",
    "# Function to apply scaling\n",
    "def apply_scaling(scaler, X_train, X_test):\n",
    "    if scaler is None:\n",
    "        return X_train, X_test\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "print(\"Scalers prepared:\")\n",
    "for name in scalers.keys():\n",
    "    print(f\"  ‚Ä¢ {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation Metrics Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics functions defined\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate multiple regression metrics\"\"\"\n",
    "    metrics = {\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'MAE': mean_absolute_error(y_true, y_pred),\n",
    "        'R¬≤': r2_score(y_true, y_pred),\n",
    "        'MAPE': mean_absolute_percentage_error(y_true, y_pred) * 100,\n",
    "        'Adjusted_R¬≤': 1 - (1 - r2_score(y_true, y_pred)) * (len(y_true) - 1) / (len(y_true) - X_train.shape[1] - 1)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def print_metrics(metrics, title=\"Metrics\"):\n",
    "    \"\"\"Print metrics in organized format\"\"\"\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    for metric, value in metrics.items():\n",
    "        if metric in ['R¬≤', 'Adjusted_R¬≤']:\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "        elif metric == 'MAPE':\n",
    "            print(f\"{metric}: {value:.2f}%\")\n",
    "        else:\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(\"Metrics functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model 1: Linear Regression with Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç TRAINING LINEAR REGRESSION MODELS\n",
      "\n",
      "üîß Training LinearRegression...\n",
      "\n",
      "=== Training ===\n",
      "RMSE: 0.0000\n",
      "MAE: 0.0000\n",
      "R¬≤: 1.0000\n",
      "MAPE: 0.00%\n",
      "Adjusted_R¬≤: 1.0000\n",
      "\n",
      "=== Test ===\n",
      "RMSE: 0.0000\n",
      "MAE: 0.0000\n",
      "R¬≤: 1.0000\n",
      "MAPE: 0.00%\n",
      "Adjusted_R¬≤: 1.0000\n",
      "CV RMSE: 0.0000 (¬±0.0000)\n",
      "\n",
      "üîß Training Ridge...\n",
      "  Best hyperparameters: {'alpha': 0.1}\n",
      "\n",
      "=== Training ===\n",
      "RMSE: 0.0000\n",
      "MAE: 0.0000\n",
      "R¬≤: 1.0000\n",
      "MAPE: 289.79%\n",
      "Adjusted_R¬≤: 1.0000\n",
      "\n",
      "=== Test ===\n",
      "RMSE: 0.0000\n",
      "MAE: 0.0000\n",
      "R¬≤: 1.0000\n",
      "MAPE: 285.23%\n",
      "Adjusted_R¬≤: 1.0000\n",
      "CV RMSE: 0.0000 (¬±0.0000)\n",
      "\n",
      "üîß Training Lasso...\n",
      "  Best hyperparameters: {'alpha': 0.001}\n",
      "\n",
      "=== Training ===\n",
      "RMSE: 0.0564\n",
      "MAE: 0.0386\n",
      "R¬≤: 0.9435\n",
      "MAPE: 24105640.33%\n",
      "Adjusted_R¬≤: 0.9435\n",
      "\n",
      "=== Test ===\n",
      "RMSE: 0.0565\n",
      "MAE: 0.0386\n",
      "R¬≤: 0.9434\n",
      "MAPE: 23726757.01%\n",
      "Adjusted_R¬≤: 0.9434\n",
      "CV RMSE: 0.0564 (¬±0.0037)\n",
      "\n",
      "üîß Training ElasticNet...\n",
      "  Best hyperparameters: {'alpha': 0.001, 'l1_ratio': 0.1}\n",
      "\n",
      "=== Training ===\n",
      "RMSE: 0.0391\n",
      "MAE: 0.0274\n",
      "R¬≤: 0.9728\n",
      "MAPE: 18765079.42%\n",
      "Adjusted_R¬≤: 0.9728\n",
      "\n",
      "=== Test ===\n",
      "RMSE: 0.0392\n",
      "MAE: 0.0274\n",
      "R¬≤: 0.9728\n",
      "MAPE: 18470137.09%\n",
      "Adjusted_R¬≤: 0.9728\n",
      "CV RMSE: 0.0391 (¬±0.0024)\n",
      "\n",
      "‚úÖ Linear models trained successfully\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç TRAINING LINEAR REGRESSION MODELS\")\n",
    "\n",
    "# Linear regression models to test\n",
    "linear_models = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'Ridge': Ridge(),\n",
    "    'Lasso': Lasso(max_iter=2000),\n",
    "    'ElasticNet': ElasticNet(max_iter=2000)\n",
    "}\n",
    "\n",
    "# Hyperparameters for search\n",
    "linear_params = {\n",
    "    'Ridge': {'alpha': [0.1, 1, 10, 100, 1000]},\n",
    "    'Lasso': {'alpha': [0.001, 0.01, 0.1, 1, 10]},\n",
    "    'ElasticNet': {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1],\n",
    "        'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Linear model results\n",
    "linear_results = {}\n",
    "\n",
    "# Use StandardScaler for linear models\n",
    "scaler = StandardScaler()\n",
    "#X_train_scaled, X_test_scaled = apply_scaling(scaler, X_train, X_test)\n",
    "X_train_scaled, X_test_scaled = X_train, X_test\n",
    "\n",
    "for model_name, model in linear_models.items():\n",
    "    print(f\"\\nüîß Training {model_name}...\")\n",
    "    \n",
    "    if model_name in linear_params:\n",
    "        # Hyperparameter search with cross-validation\n",
    "        grid_search = GridSearchCV(\n",
    "            model, linear_params[model_name], \n",
    "            cv=5, scoring='neg_mean_squared_error', \n",
    "            n_jobs=-1\n",
    "        )\n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        print(f\"  Best hyperparameters: {grid_search.best_params_}\")\n",
    "    else:\n",
    "        # Model without hyperparameters\n",
    "        best_model = model\n",
    "        best_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = best_model.predict(X_train_scaled)\n",
    "    y_test_pred = best_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Metrics\n",
    "    train_metrics = calculate_metrics(y_train, y_train_pred)\n",
    "    test_metrics = calculate_metrics(y_test, y_test_pred)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(best_model, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    cv_rmse = np.sqrt(-cv_scores.mean())\n",
    "    cv_rmse_std = np.sqrt(cv_scores.std())\n",
    "    \n",
    "    linear_results[model_name] = {\n",
    "        'model': best_model,\n",
    "        'train_metrics': train_metrics,\n",
    "        'test_metrics': test_metrics,\n",
    "        'cv_rmse': cv_rmse,\n",
    "        'cv_rmse_std': cv_rmse_std,\n",
    "        'predictions': {'train': y_train_pred, 'test': y_test_pred}\n",
    "    }\n",
    "    \n",
    "    print_metrics(train_metrics, \"Training\")\n",
    "    print_metrics(test_metrics, \"Test\")\n",
    "    print(f\"CV RMSE: {cv_rmse:.4f} (¬±{cv_rmse_std:.4f})\")\n",
    "\n",
    "print(\"\\n‚úÖ Linear models trained successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model 2: Support Vector Regression (SVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç TRAINING SUPPORT VECTOR REGRESSION\n",
      "\n",
      "üîß Training SVR with linear kernel...\n"
     ]
    },
    {
     "ename": "TerminatedWorkerError",
     "evalue": "A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n\nThe exit codes of the workers are {SIGKILL(-9)}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTerminatedWorkerError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ff447bae2c1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m     )\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_robust\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mbest_svr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    849\u001b[0m                     )\n\u001b[1;32m    850\u001b[0m                     for (cand_idx, parameters), (split_idx, (train, test)) in product(\n\u001b[0;32m--> 851\u001b[0;31m                         \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m                     )\n\u001b[1;32m    853\u001b[0m                 )\n",
      "\u001b[0;32m/opt/local/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1950\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1952\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1595\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1697\u001b[0m             \u001b[0;31m# worker traceback.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1698\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aborting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1699\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_error_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1700\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1732\u001b[0m         \u001b[0;31m# called directly or if the generator is gc'ed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1733\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merror_job\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1734\u001b[0;31m             \u001b[0merror_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1736\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_warn_exit_early\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0;31m# callback thread, and is stored internally. It's just waiting to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0;31m# be returned.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_return_or_raise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;31m# For other backends, the main thread needs to run the retrieval step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_ERROR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTerminatedWorkerError\u001b[0m: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n\nThe exit codes of the workers are {SIGKILL(-9)}"
     ]
    }
   ],
   "source": [
    "print(\"üîç TRAINING SUPPORT VECTOR REGRESSION\")\n",
    "\n",
    "# Hyperparameters for SVR\n",
    "svr_params = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "    'epsilon': [0.01, 0.1, 0.2, 0.5]\n",
    "}\n",
    "\n",
    "# Different kernels to test\n",
    "svr_kernels = ['linear', 'rbf', 'poly']\n",
    "svr_results = {}\n",
    "\n",
    "# Use RobustScaler for SVR (more robust to outliers)\n",
    "robust_scaler = RobustScaler()\n",
    "X_train_robust, X_test_robust = apply_scaling(robust_scaler, X_train, X_test)\n",
    "\n",
    "for kernel in svr_kernels:\n",
    "    print(f\"\\nüîß Training SVR with {kernel} kernel...\")\n",
    "    \n",
    "    # Create SVR model\n",
    "    svr_model = SVR(kernel=kernel)\n",
    "    \n",
    "    # Adjust parameters according to kernel\n",
    "    current_params = svr_params.copy()\n",
    "    if kernel == 'linear':\n",
    "        current_params.pop('gamma')  # Gamma not relevant for linear kernel\n",
    "    elif kernel == 'poly':\n",
    "        current_params['degree'] = [2, 3, 4]  # Add degree for polynomial\n",
    "    \n",
    "    # Hyperparameter search\n",
    "    grid_search = GridSearchCV(\n",
    "        svr_model, current_params, \n",
    "        cv=5, scoring='neg_mean_squared_error', \n",
    "        n_jobs=-1, verbose=0\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train_robust, y_train)\n",
    "    best_svr = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"  Best hyperparameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = best_svr.predict(X_train_robust)\n",
    "    y_test_pred = best_svr.predict(X_test_robust)\n",
    "    \n",
    "    # Metrics\n",
    "    train_metrics = calculate_metrics(y_train, y_train_pred)\n",
    "    test_metrics = calculate_metrics(y_test, y_test_pred)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(best_svr, X_train_robust, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    cv_rmse = np.sqrt(-cv_scores.mean())\n",
    "    cv_rmse_std = np.sqrt(cv_scores.std())\n",
    "    \n",
    "    svr_results[f'SVR_{kernel}'] = {\n",
    "        'model': best_svr,\n",
    "        'train_metrics': train_metrics,\n",
    "        'test_metrics': test_metrics,\n",
    "        'cv_rmse': cv_rmse,\n",
    "        'cv_rmse_std': cv_rmse_std,\n",
    "        'predictions': {'train': y_train_pred, 'test': y_test_pred}\n",
    "    }\n",
    "    \n",
    "    print_metrics(train_metrics, \"Training\")\n",
    "    print_metrics(test_metrics, \"Test\")\n",
    "    print(f\"CV RMSE: {cv_rmse:.4f} (¬±{cv_rmse_std:.4f})\")\n",
    "\n",
    "print(\"\\n‚úÖ SVR models trained successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model 3: XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç TRAINING XGBOOST REGRESSOR\")\n",
    "\n",
    "# Hyperparameters for XGBoost\n",
    "xgb_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Create XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    random_state=42,\n",
    "    verbosity=0,\n",
    "    eval_metric='rmse'\n",
    ")\n",
    "\n",
    "# XGBoost doesn't require scaling, use original data\n",
    "print(\"üîß Training XGBoost (may take several minutes)...\")\n",
    "\n",
    "# Random search to reduce computation time\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Use RandomizedSearchCV instead of GridSearchCV for efficiency\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgb_model, xgb_params,\n",
    "    n_iter=50,  # Number of combinations to test\n",
    "    cv=5, scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1, random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "best_xgb = random_search.best_estimator_\n",
    "\n",
    "print(f\"  Best hyperparameters: {random_search.best_params_}\")\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = best_xgb.predict(X_train)\n",
    "y_test_pred = best_xgb.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "train_metrics = calculate_metrics(y_train, y_train_pred)\n",
    "test_metrics = calculate_metrics(y_test, y_test_pred)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(best_xgb, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_rmse = np.sqrt(-cv_scores.mean())\n",
    "cv_rmse_std = np.sqrt(cv_scores.std())\n",
    "\n",
    "xgb_results = {\n",
    "    'model': best_xgb,\n",
    "    'train_metrics': train_metrics,\n",
    "    'test_metrics': test_metrics,\n",
    "    'cv_rmse': cv_rmse,\n",
    "    'cv_rmse_std': cv_rmse_std,\n",
    "    'predictions': {'train': y_train_pred, 'test': y_test_pred}\n",
    "}\n",
    "\n",
    "print_metrics(train_metrics, \"Training\")\n",
    "print_metrics(test_metrics, \"Test\")\n",
    "print(f\"CV RMSE: {cv_rmse:.4f} (¬±{cv_rmse_std:.4f})\")\n",
    "\n",
    "print(\"\\n‚úÖ XGBoost model trained successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results\n",
    "#all_results = {**linear_results, **svr_results, 'XGBoost': xgb_results}\n",
    "all_results = {**linear_results, **svr_results}\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "for model_name, results in all_results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Train_RMSE': results['train_metrics']['RMSE'],\n",
    "        'Test_RMSE': results['test_metrics']['RMSE'],\n",
    "        'Train_R¬≤': results['train_metrics']['R¬≤'],\n",
    "        'Test_R¬≤': results['test_metrics']['R¬≤'],\n",
    "        'Train_MAE': results['train_metrics']['MAE'],\n",
    "        'Test_MAE': results['test_metrics']['MAE'],\n",
    "        'CV_RMSE': results['cv_rmse'],\n",
    "        'CV_RMSE_Std': results['cv_rmse_std'],\n",
    "        'Overfitting': results['train_metrics']['RMSE'] - results['test_metrics']['RMSE']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('Test_R¬≤', ascending=False)\n",
    "\n",
    "print(\"=== MODEL COMPARISON ===\")\n",
    "display(comparison_df.round(4))\n",
    "\n",
    "# Identify best model\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
    "print(f\"   Test R¬≤: {comparison_df.iloc[0]['Test_R¬≤']:.4f}\")\n",
    "print(f\"   Test RMSE: {comparison_df.iloc[0]['Test_RMSE']:.4f}\")\n",
    "print(f\"   CV RMSE: {comparison_df.iloc[0]['CV_RMSE']:.4f} (¬±{comparison_df.iloc[0]['CV_RMSE_Std']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# R¬≤ Score\n",
    "ax1 = axes[0, 0]\n",
    "x_pos = np.arange(len(comparison_df))\n",
    "ax1.bar(x_pos, comparison_df['Test_R¬≤'], alpha=0.7, color='lightblue')\n",
    "ax1.set_xlabel('Models')\n",
    "ax1.set_ylabel('R¬≤ Score')\n",
    "ax1.set_title('R¬≤ Score on Test Set')\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(comparison_df['Model'], rotation=45)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# RMSE\n",
    "ax2 = axes[0, 1]\n",
    "ax2.bar(x_pos, comparison_df['Test_RMSE'], alpha=0.7, color='lightcoral')\n",
    "ax2.set_xlabel('Models')\n",
    "ax2.set_ylabel('RMSE')\n",
    "ax2.set_title('RMSE on Test Set')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(comparison_df['Model'], rotation=45)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Overfitting (Train RMSE - Test RMSE)\n",
    "ax3 = axes[1, 0]\n",
    "colors = ['red' if x > 0 else 'green' for x in comparison_df['Overfitting']]\n",
    "ax3.bar(x_pos, comparison_df['Overfitting'], alpha=0.7, color=colors)\n",
    "ax3.set_xlabel('Models')\n",
    "ax3.set_ylabel('Overfitting (Train RMSE - Test RMSE)')\n",
    "ax3.set_title('Overfitting Analysis')\n",
    "ax3.set_xticks(x_pos)\n",
    "ax3.set_xticklabels(comparison_df['Model'], rotation=45)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Cross-Validation RMSE with error bars\n",
    "ax4 = axes[1, 1]\n",
    "ax4.bar(x_pos, comparison_df['CV_RMSE'], alpha=0.7, color='lightgreen',\n",
    "        yerr=comparison_df['CV_RMSE_Std'], capsize=5)\n",
    "ax4.set_xlabel('Models')\n",
    "ax4.set_ylabel('CV RMSE')\n",
    "ax4.set_title('Cross-Validation RMSE')\n",
    "ax4.set_xticks(x_pos)\n",
    "ax4.set_xticklabels(comparison_df['Model'], rotation=45)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç ANALYZING FEATURE IMPORTANCE\")\n",
    "\n",
    "# Function to get feature importance\n",
    "def get_feature_importance(model, model_name, X_test, y_test):\n",
    "    \"\"\"Get feature importance according to model type\"\"\"\n",
    "    \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        # Models with feature_importances_ (XGBoost, RandomForest, etc.)\n",
    "        importance = model.feature_importances_\n",
    "        method = 'Built-in Feature Importance'\n",
    "    \n",
    "    elif hasattr(model, 'coef_'):\n",
    "        # Linear models\n",
    "        importance = np.abs(model.coef_)\n",
    "        method = 'Absolute Coefficients'\n",
    "    \n",
    "    else:\n",
    "        # Use permutation importance for other models\n",
    "        perm_importance = permutation_importance(\n",
    "            model, X_test, y_test, \n",
    "            n_repeats=10, random_state=42, \n",
    "            scoring='neg_mean_squared_error'\n",
    "        )\n",
    "        importance = perm_importance.importances_mean\n",
    "        method = 'Permutation Importance'\n",
    "    \n",
    "    return importance, method\n",
    "\n",
    "# Analyze importance for top 3 models\n",
    "top_3_models = comparison_df.head(3)\n",
    "importance_results = {}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for i, (_, row) in enumerate(top_3_models.iterrows()):\n",
    "    model_name = row['Model']\n",
    "    model_obj = all_results[model_name]['model']\n",
    "    \n",
    "    # Prepare test data according to model\n",
    "    if model_name.startswith('SVR'):\n",
    "        X_test_prepared = robust_scaler.transform(X_test)\n",
    "    elif model_name in ['LinearRegression', 'Ridge', 'Lasso', 'ElasticNet']:\n",
    "        X_test_prepared = scaler.transform(X_test)\n",
    "    else:  # XGBoost\n",
    "        X_test_prepared = X_test\n",
    "    \n",
    "    importance, method = get_feature_importance(model_obj, model_name, X_test_prepared, y_test)\n",
    "    \n",
    "    # Normalize importance\n",
    "    importance_normalized = importance / importance.sum()\n",
    "    \n",
    "    # Create DataFrame for visualization\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_cols,\n",
    "        'Importance': importance_normalized\n",
    "    }).sort_values('Importance', ascending=True)\n",
    "    \n",
    "    importance_results[model_name] = importance_df\n",
    "    \n",
    "    # Plot\n",
    "    ax = axes[i]\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(importance_df)))\n",
    "    bars = ax.barh(importance_df['Feature'], importance_df['Importance'], color=colors)\n",
    "    ax.set_title(f'{model_name}\\n({method})')\n",
    "    ax.set_xlabel('Normalized Importance')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add values on bars\n",
    "    for bar, importance in zip(bars, importance_df['Importance']):\n",
    "        ax.text(bar.get_width() + 0.005, bar.get_y() + bar.get_height()/2, \n",
    "                f'{importance:.3f}', ha='left', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print feature importance ranking\n",
    "print(\"\\n=== FEATURE IMPORTANCE RANKING ===\")\n",
    "for model_name, imp_df in importance_results.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    for i, (_, row) in enumerate(imp_df.sort_values('Importance', ascending=False).iterrows(), 1):\n",
    "        print(f\"  {i}. {row['Feature']}: {row['Importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Residual Analysis of Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model for detailed analysis\n",
    "best_model_results = all_results[best_model_name]\n",
    "best_model = best_model_results['model']\n",
    "y_train_pred = best_model_results['predictions']['train']\n",
    "y_test_pred = best_model_results['predictions']['test']\n",
    "\n",
    "print(f\"üîç RESIDUAL ANALYSIS - {best_model_name}\")\n",
    "\n",
    "# Calculate residuals\n",
    "train_residuals = y_train - y_train_pred\n",
    "test_residuals = y_test - y_test_pred\n",
    "\n",
    "# Create residual analysis plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Predictions vs Actual Values (Train)\n",
    "ax1 = axes[0, 0]\n",
    "ax1.scatter(y_train, y_train_pred, alpha=0.6, s=20)\n",
    "min_val = min(y_train.min(), y_train_pred.min())\n",
    "max_val = max(y_train.max(), y_train_pred.max())\n",
    "ax1.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
    "ax1.set_xlabel('Actual Values')\n",
    "ax1.set_ylabel('Predictions')\n",
    "ax1.set_title('Train: Predictions vs Actual')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Predictions vs Actual Values (Test)\n",
    "ax2 = axes[0, 1]\n",
    "ax2.scatter(y_test, y_test_pred, alpha=0.6, s=20, color='orange')\n",
    "min_val = min(y_test.min(), y_test_pred.min())\n",
    "max_val = max(y_test.max(), y_test_pred.max())\n",
    "ax2.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
    "ax2.set_xlabel('Actual Values')\n",
    "ax2.set_ylabel('Predictions')\n",
    "ax2.set_title('Test: Predictions vs Actual')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Residuals vs Predictions (Test)\n",
    "ax3 = axes[0, 2]\n",
    "ax3.scatter(y_test_pred, test_residuals, alpha=0.6, s=20, color='green')\n",
    "ax3.axhline(y=0, color='r', linestyle='--')\n",
    "ax3.set_xlabel('Predictions')\n",
    "ax3.set_ylabel('Residuals')\n",
    "ax3.set_title('Residuals vs Predictions (Test)')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Histogram of Residuals (Train)\n",
    "ax4 = axes[1, 0]\n",
    "ax4.hist(train_residuals, bins=30, alpha=0.7, density=True, color='skyblue')\n",
    "ax4.axvline(train_residuals.mean(), color='red', linestyle='--', label=f'Mean: {train_residuals.mean():.3f}')\n",
    "ax4.set_xlabel('Residuals')\n",
    "ax4.set_ylabel('Density')\n",
    "ax4.set_title('Residual Distribution (Train)')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Histogram of Residuals (Test)\n",
    "ax5 = axes[1, 1]\n",
    "ax5.hist(test_residuals, bins=30, alpha=0.7, density=True, color='orange')\n",
    "ax5.axvline(test_residuals.mean(), color='red', linestyle='--', label=f'Mean: {test_residuals.mean():.3f}')\n",
    "ax5.set_xlabel('Residuals')\n",
    "ax5.set_ylabel('Density')\n",
    "ax5.set_title('Residual Distribution (Test)')\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Q-Q Plot of Residuals (Test)\n",
    "ax6 = axes[1, 2]\n",
    "stats.probplot(test_residuals, dist=\"norm\", plot=ax6)\n",
    "ax6.set_title('Q-Q Plot of Residuals (Test)')\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Residual statistics\n",
    "print(f\"\\n=== RESIDUAL STATISTICS ===\")\n",
    "print(f\"Train:\")\n",
    "print(f\"  Mean: {train_residuals.mean():.6f}\")\n",
    "print(f\"  Std: {train_residuals.std():.4f}\")\n",
    "print(f\"  Skewness: {stats.skew(train_residuals):.4f}\")\n",
    "print(f\"  Kurtosis: {stats.kurtosis(train_residuals):.4f}\")\n",
    "\n",
    "print(f\"\\nTest:\")\n",
    "print(f\"  Mean: {test_residuals.mean():.6f}\")\n",
    "print(f\"  Std: {test_residuals.std():.4f}\")\n",
    "print(f\"  Skewness: {stats.skew(test_residuals):.4f}\")\n",
    "print(f\"  Kurtosis: {stats.kurtosis(test_residuals):.4f}\")\n",
    "\n",
    "# Normality test for residuals\n",
    "_, p_value = stats.shapiro(test_residuals[:1000] if len(test_residuals) > 1000 else test_residuals)\n",
    "print(f\"\\nNormality Test (Shapiro-Wilk): p-value = {p_value:.6f}\")\n",
    "print(f\"Residuals are normal: {'Yes' if p_value > 0.05 else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Learning and Validation Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "print(f\"üîç GENERATING LEARNING CURVES FOR {best_model_name}\")\n",
    "\n",
    "# Prepare data according to best model\n",
    "if best_model_name.startswith('SVR'):\n",
    "    X_for_curves = robust_scaler.fit_transform(X_train)\n",
    "elif best_model_name in ['LinearRegression', 'Ridge', 'Lasso', 'ElasticNet']:\n",
    "    X_for_curves = scaler.fit_transform(X_train)\n",
    "else:  # XGBoost\n",
    "    X_for_curves = X_train\n",
    "\n",
    "# Generate learning curves\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_model, X_for_curves, y_train,\n",
    "    cv=5, n_jobs=-1,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "# Convert to RMSE\n",
    "train_rmse = np.sqrt(-train_scores)\n",
    "val_rmse = np.sqrt(-val_scores)\n",
    "\n",
    "# Calculate means and standard deviations\n",
    "train_rmse_mean = train_rmse.mean(axis=1)\n",
    "train_rmse_std = train_rmse.std(axis=1)\n",
    "val_rmse_mean = val_rmse.mean(axis=1)\n",
    "val_rmse_std = val_rmse.std(axis=1)\n",
    "\n",
    "# Learning curves plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(train_sizes, train_rmse_mean, 'o-', color='blue', label='Training RMSE')\n",
    "plt.fill_between(train_sizes, train_rmse_mean - train_rmse_std, \n",
    "                 train_rmse_mean + train_rmse_std, alpha=0.1, color='blue')\n",
    "\n",
    "plt.plot(train_sizes, val_rmse_mean, 'o-', color='red', label='Validation RMSE')\n",
    "plt.fill_between(train_sizes, val_rmse_mean - val_rmse_std, \n",
    "                 val_rmse_mean + val_rmse_std, alpha=0.1, color='red')\n",
    "\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title(f'Learning Curves - {best_model_name}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Learning curves analysis\n",
    "final_gap = val_rmse_mean[-1] - train_rmse_mean[-1]\n",
    "print(f\"\\n=== LEARNING CURVES ANALYSIS ===\")\n",
    "print(f\"Final training RMSE: {train_rmse_mean[-1]:.4f} (¬±{train_rmse_std[-1]:.4f})\")\n",
    "print(f\"Final validation RMSE: {val_rmse_mean[-1]:.4f} (¬±{val_rmse_std[-1]:.4f})\")\n",
    "print(f\"Final gap (Val - Train): {final_gap:.4f}\")\n",
    "\n",
    "if final_gap > 0.1 * train_rmse_mean[-1]:\n",
    "    print(\"‚ö†Ô∏è Possible overfitting detected\")\n",
    "elif final_gap < 0:\n",
    "    print(\"‚ö†Ô∏è Possible underfitting or easier validation data\")\n",
    "else:\n",
    "    print(\"‚úÖ Good balance between bias and variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Weight Optimization to Maximize Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize, differential_evolution\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print(\"üéØ WEIGHT OPTIMIZATION TO MAXIMIZE OUTPUT\")\n",
    "\n",
    "# Objective function to maximize model prediction\n",
    "def objective_function(weights, model, scaler=None, feature_names=None):\n",
    "    \"\"\"Objective function: we want to maximize model output\"\"\"\n",
    "    weights_reshaped = weights.reshape(1, -1)\n",
    "    \n",
    "    # Apply scaling if necessary\n",
    "    if scaler is not None:\n",
    "        weights_scaled = scaler.transform(weights_reshaped)\n",
    "    else:\n",
    "        weights_scaled = weights_reshaped\n",
    "    \n",
    "    # Predict (we want to maximize, so return negative)\n",
    "    prediction = model.predict(weights_scaled)[0]\n",
    "    return -prediction  # Negative because minimize searches for minimum\n",
    "\n",
    "# Define feature bounds based on data\n",
    "feature_bounds = []\n",
    "print(\"\\nFeature bounds based on data:\")\n",
    "for i, feature in enumerate(feature_cols):\n",
    "    min_val = X[feature].min()\n",
    "    max_val = X[feature].max()\n",
    "    feature_bounds.append((min_val, max_val))\n",
    "    print(f\"  {feature}: [{min_val:.2f}, {max_val:.2f}]\")\n",
    "\n",
    "# Optimize for best model\n",
    "print(f\"\\nüîß Optimizing with {best_model_name}...\")\n",
    "\n",
    "# Prepare scaler if necessary\n",
    "optimization_scaler = None\n",
    "if best_model_name.startswith('SVR'):\n",
    "    optimization_scaler = RobustScaler().fit(X_train)\n",
    "elif best_model_name in ['LinearRegression', 'Ridge', 'Lasso', 'ElasticNet']:\n",
    "    optimization_scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "# Use differential evolution (more robust for non-convex problems)\n",
    "result = differential_evolution(\n",
    "    objective_function,\n",
    "    feature_bounds,\n",
    "    args=(best_model, optimization_scaler, feature_cols),\n",
    "    seed=42,\n",
    "    maxiter=1000,\n",
    "    popsize=15\n",
    ")\n",
    "\n",
    "optimal_weights = result.x\n",
    "optimal_prediction = -result.fun  # Convert back (remove negative)\n",
    "\n",
    "print(f\"\\n‚úÖ OPTIMIZATION COMPLETED\")\n",
    "print(f\"Maximum predicted value: {optimal_prediction:.4f}\")\n",
    "print(f\"Number of evaluations: {result.nfev}\")\n",
    "print(f\"Success: {result.success}\")\n",
    "\n",
    "# Show optimal weights\n",
    "print(f\"\\n=== OPTIMAL WEIGHTS TO MAXIMIZE OUTPUT ===\")\n",
    "optimal_df = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Optimal_Value': optimal_weights,\n",
    "    'Data_Min': [X[col].min() for col in feature_cols],\n",
    "    'Data_Max': [X[col].max() for col in feature_cols],\n",
    "    'Percentile_in_Data': [stats.percentileofscore(X[col], val) for col, val in zip(feature_cols, optimal_weights)]\n",
    "})\n",
    "\n",
    "display(optimal_df.round(4))\n",
    "\n",
    "# Compare with existing dataset examples\n",
    "print(f\"\\n=== COMPARISON WITH EXISTING DATA ===\")\n",
    "# Find top 5 samples with highest target values\n",
    "top_5_indices = y.nlargest(5).index\n",
    "top_5_predictions = []\n",
    "\n",
    "for idx in top_5_indices:\n",
    "    sample = X.loc[idx].values.reshape(1, -1)\n",
    "    if optimization_scaler is not None:\n",
    "        sample_scaled = optimization_scaler.transform(sample)\n",
    "    else:\n",
    "        sample_scaled = sample\n",
    "    pred = best_model.predict(sample_scaled)[0]\n",
    "    top_5_predictions.append(pred)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Type': ['Dataset Top 1', 'Dataset Top 2', 'Dataset Top 3', 'Dataset Top 4', 'Dataset Top 5', 'OPTIMAL WEIGHTS'],\n",
    "    'Prediction': top_5_predictions + [optimal_prediction],\n",
    "    'Actual_Target': list(y.loc[top_5_indices]) + ['N/A']\n",
    "})\n",
    "\n",
    "display(comparison_df.round(4))\n",
    "\n",
    "improvement = optimal_prediction - max(top_5_predictions)\n",
    "print(f\"\\nImprovement over best dataset case: {improvement:.4f} ({improvement/max(top_5_predictions)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Optimal Weights Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal weights visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Optimal values vs data ranges\n",
    "ax1 = axes[0, 0]\n",
    "x_pos = np.arange(len(feature_cols))\n",
    "ax1.bar(x_pos, optimal_weights, alpha=0.7, color='gold', label='Optimal Values')\n",
    "\n",
    "# Add lines for min/max of data\n",
    "mins = [X[col].min() for col in feature_cols]\n",
    "maxs = [X[col].max() for col in feature_cols]\n",
    "ax1.errorbar(x_pos, optimal_weights, \n",
    "            yerr=[np.array(optimal_weights) - np.array(mins), \n",
    "                  np.array(maxs) - np.array(optimal_weights)], \n",
    "            fmt='none', color='red', alpha=0.5, capsize=5)\n",
    "\n",
    "ax1.set_xlabel('Features')\n",
    "ax1.set_ylabel('Values')\n",
    "ax1.set_title('Optimal Values vs Data Ranges')\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(feature_cols, rotation=45)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Percentiles of optimal values\n",
    "ax2 = axes[0, 1]\n",
    "percentiles = optimal_df['Percentile_in_Data']\n",
    "colors = ['red' if p > 90 else 'orange' if p > 75 else 'yellow' if p > 50 else 'lightblue' for p in percentiles]\n",
    "bars = ax2.bar(x_pos, percentiles, color=colors, alpha=0.7)\n",
    "ax2.axhline(y=50, color='gray', linestyle='--', alpha=0.7, label='Median')\n",
    "ax2.axhline(y=90, color='red', linestyle='--', alpha=0.7, label='90th Percentile')\n",
    "ax2.set_xlabel('Features')\n",
    "ax2.set_ylabel('Percentile')\n",
    "ax2.set_title('Percentiles of Optimal Values')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(feature_cols, rotation=45)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add labels with percentiles\n",
    "for bar, p in zip(bars, percentiles):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "            f'{p:.0f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 3. Comparison with best dataset samples\n",
    "ax3 = axes[1, 0]\n",
    "comparison_values = list(comparison_df['Prediction'][:-1]) + [optimal_prediction]\n",
    "labels = ['Top 1', 'Top 2', 'Top 3', 'Top 4', 'Top 5', 'Optimal']\n",
    "colors = ['lightblue'] * 5 + ['gold']\n",
    "\n",
    "bars = ax3.bar(labels, comparison_values, color=colors, alpha=0.7)\n",
    "ax3.set_xlabel('Samples')\n",
    "ax3.set_ylabel('Prediction')\n",
    "ax3.set_title('Comparison: Best Samples vs Optimal')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Add labels with values\n",
    "for bar, val in zip(bars, comparison_values):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(comparison_values)*0.01, \n",
    "            f'{val:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 4. Radar chart comparing optimal vs dataset average\n",
    "ax4 = axes[1, 1]\n",
    "angles = np.linspace(0, 2*np.pi, len(feature_cols), endpoint=False).tolist()\n",
    "angles += angles[:1]  # Complete the circle\n",
    "\n",
    "# Normalize values for radar chart\n",
    "scaler_radar = MinMaxScaler()\n",
    "data_for_radar = np.column_stack([optimal_weights, X[feature_cols].mean().values])\n",
    "normalized_data = scaler_radar.fit_transform(data_for_radar)\n",
    "\n",
    "optimal_normalized = normalized_data[:, 0].tolist()\n",
    "mean_normalized = normalized_data[:, 1].tolist()\n",
    "\n",
    "optimal_normalized += optimal_normalized[:1]\n",
    "mean_normalized += mean_normalized[:1]\n",
    "\n",
    "ax4 = plt.subplot(2, 2, 4, projection='polar')\n",
    "ax4.plot(angles, optimal_normalized, 'o-', linewidth=2, label='Optimal Values', color='gold')\n",
    "ax4.fill(angles, optimal_normalized, alpha=0.25, color='gold')\n",
    "ax4.plot(angles, mean_normalized, 'o-', linewidth=2, label='Dataset Average', color='blue')\n",
    "ax4.fill(angles, mean_normalized, alpha=0.25, color='blue')\n",
    "\n",
    "ax4.set_xticks(angles[:-1])\n",
    "ax4.set_xticklabels(feature_cols)\n",
    "ax4.set_ylim(0, 1)\n",
    "ax4.set_title('Radar Comparison: Optimal vs Average', y=1.08)\n",
    "ax4.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Final Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üéØ MACHINE LEARNING PROJECT FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä DATASET:\")\n",
    "print(f\"   ‚Ä¢ Size: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "print(f\"   ‚Ä¢ Features: {', '.join(feature_cols)}\")\n",
    "print(f\"   ‚Ä¢ Split: {X_train.shape[0]} train / {X_test.shape[0]} test\")\n",
    "\n",
    "print(f\"\\nü§ñ MODELS EVALUATED:\")\n",
    "models_tested = len(all_results)\n",
    "print(f\"   ‚Ä¢ Total models: {models_tested}\")\n",
    "print(f\"   ‚Ä¢ Linear Regression: {len(linear_results)} variants\")\n",
    "print(f\"   ‚Ä¢ Support Vector Regression: {len(svr_results)} kernels\")\n",
    "print(f\"   ‚Ä¢ XGBoost: 1 model with optimized hyperparameters\")\n",
    "\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
    "best_test_metrics = best_model_results['test_metrics']\n",
    "print(f\"   ‚Ä¢ R¬≤ Score: {best_test_metrics['R¬≤']:.4f}\")\n",
    "print(f\"   ‚Ä¢ RMSE: {best_test_metrics['RMSE']:.4f}\")\n",
    "print(f\"   ‚Ä¢ MAE: {best_test_metrics['MAE']:.4f}\")\n",
    "print(f\"   ‚Ä¢ MAPE: {best_test_metrics['MAPE']:.2f}%\")\n",
    "print(f\"   ‚Ä¢ CV RMSE: {best_model_results['cv_rmse']:.4f} (¬±{best_model_results['cv_rmse_std']:.4f})\")\n",
    "\n",
    "print(f\"\\nüéØ MOST IMPORTANT FEATURES:\")\n",
    "if best_model_name in importance_results:\n",
    "    top_features = importance_results[best_model_name].sort_values('Importance', ascending=False).head(3)\n",
    "    for i, (_, row) in enumerate(top_features.iterrows(), 1):\n",
    "        print(f\"   {i}. {row['Feature']}: {row['Importance']:.4f}\")\n",
    "\n",
    "print(f\"\\n‚ö° OPTIMAL WEIGHTS FOR MAXIMIZATION:\")\n",
    "print(f\"   ‚Ä¢ Maximum predicted value: {optimal_prediction:.4f}\")\n",
    "print(f\"   ‚Ä¢ Improvement over best sample: {improvement:.4f} ({improvement/max(top_5_predictions)*100:.2f}%)\")\n",
    "print(f\"   ‚Ä¢ Features to maximize:\")\n",
    "high_percentile_features = optimal_df[optimal_df['Percentile_in_Data'] > 75]\n",
    "for _, row in high_percentile_features.iterrows():\n",
    "    print(f\"     - {row['Feature']}: {row['Optimal_Value']:.2f} (percentile {row['Percentile_in_Data']:.0f})\")\n",
    "\n",
    "print(f\"\\nüìà MODEL QUALITY ANALYSIS:\")\n",
    "overfitting_score = best_model_results['train_metrics']['RMSE'] - best_model_results['test_metrics']['RMSE']\n",
    "if abs(overfitting_score) < 0.1 * best_model_results['test_metrics']['RMSE']:\n",
    "    print(f\"   ‚Ä¢ ‚úÖ Good balance between bias and variance\")\n",
    "elif overfitting_score > 0:\n",
    "    print(f\"   ‚Ä¢ ‚ö†Ô∏è Slight overfitting detected (difference: {overfitting_score:.4f})\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ ‚ö†Ô∏è Possible underfitting\")\n",
    "\n",
    "if best_test_metrics['R¬≤'] > 0.8:\n",
    "    quality = \"Excellent\"\n",
    "elif best_test_metrics['R¬≤'] > 0.6:\n",
    "    quality = \"Good\"\n",
    "elif best_test_metrics['R¬≤'] > 0.4:\n",
    "    quality = \"Fair\"\n",
    "else:\n",
    "    quality = \"Needs improvement\"\n",
    "    \n",
    "print(f\"   ‚Ä¢ Fit quality: {quality} (R¬≤ = {best_test_metrics['R¬≤']:.4f})\")\n",
    "\n",
    "print(f\"\\nüî¨ RECOMMENDATIONS:\")\n",
    "print(f\"   ‚Ä¢ To maximize output, use the optimal weights found\")\n",
    "print(f\"   ‚Ä¢ Focus on the highest importance features identified\")\n",
    "\n",
    "if best_test_metrics['R¬≤'] < 0.8:\n",
    "    print(f\"   ‚Ä¢ Consider additional feature engineering to improve R¬≤\")\n",
    "    print(f\"   ‚Ä¢ Explore more complex models (Neural Networks, Ensemble methods)\")\n",
    "    \n",
    "if abs(overfitting_score) > 0.1 * best_model_results['test_metrics']['RMSE']:\n",
    "    print(f\"   ‚Ä¢ Consider additional regularization techniques\")\n",
    "    print(f\"   ‚Ä¢ Increase training dataset size if possible\")\n",
    "\n",
    "print(f\"   ‚Ä¢ Use cross-validation for production decisions\")\n",
    "print(f\"   ‚Ä¢ Monitor model performance on new data\")\n",
    "\n",
    "print(f\"\\nüíæ FILES GENERATED:\")\n",
    "print(f\"   ‚Ä¢ 01_exploratory_data_analysis_EN.ipynb: Complete exploratory analysis\")\n",
    "print(f\"   ‚Ä¢ 02_model_training_evaluation_EN.ipynb: Model training and evaluation\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚ú® PROJECT COMPLETED SUCCESSFULLY ‚ú®\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
