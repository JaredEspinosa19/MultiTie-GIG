{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis Exploratorio de Datos (EDA)\n",
    "Este notebook contiene el análisis exploratorio de datos para el proyecto de regresión con ML.\n",
    "\n",
    "## Objetivos:\n",
    "- Cargar y explorar el dataset\n",
    "- Analizar distribuciones de variables\n",
    "- Identificar correlaciones\n",
    "- Detectar valores atípicos y faltantes\n",
    "- Preparar los datos para el modelado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"Librerías importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cargar el dataset\n# NOTA: Reemplaza 'tu_dataset.csv' con la ruta real de tu archivo\nfile_path = 'tu_dataset.csv'\n\ntry:\n    df = pd.read_csv(file_path)\n    print(f\"Dataset cargado exitosamente: {df.shape[0]} filas y {df.shape[1]} columnas\")\nexcept FileNotFoundError:\n    print(\"Archivo no encontrado. Por favor, ajusta la variable 'file_path' con la ruta correcta.\")\n    # Crear dataset de ejemplo para demostración con 10 características\n    np.random.seed(42)\n    n_samples = 1000\n    df = pd.DataFrame({\n        'feature_1': np.random.normal(50, 15, n_samples),\n        'feature_2': np.random.exponential(2, n_samples),\n        'feature_3': np.random.uniform(0, 100, n_samples),\n        'feature_4': np.random.gamma(2, 2, n_samples),\n        'feature_5': np.random.beta(2, 5, n_samples) * 100,\n        'feature_6': np.random.lognormal(1, 0.5, n_samples),\n        'feature_7': np.random.weibull(1.5, n_samples) * 50,\n        'feature_8': np.random.poisson(5, n_samples),\n        'feature_9': np.random.triangular(0, 50, 100, n_samples),\n        'feature_10': np.random.pareto(3, n_samples) * 10,\n        'target': np.random.normal(75, 20, n_samples)\n    })\n    # Agregar correlación artificial con múltiples características\n    df['target'] = (0.25 * df['feature_1'] + 0.15 * df['feature_3'] + 0.1 * df['feature_4'] + \n                   0.08 * df['feature_6'] + 0.12 * df['feature_7'] + 0.05 * df['feature_9'] + \n                   np.random.normal(0, 10, n_samples))\n    print(f\"Dataset de ejemplo creado: {df.shape[0]} filas y {df.shape[1]} columnas\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Información General del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Información básica del dataset\n",
    "print(\"=== INFORMACIÓN GENERAL ===\")\n",
    "print(f\"Forma del dataset: {df.shape}\")\n",
    "print(f\"\\nTipos de datos:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nMemoria utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeras y últimas filas\n",
    "print(\"=== PRIMERAS 5 FILAS ===\")\n",
    "display(df.head())\n",
    "print(\"\\n=== ÚLTIMAS 5 FILAS ===\")\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estadísticas descriptivas\n",
    "print(\"=== ESTADÍSTICAS DESCRIPTIVAS ===\")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Análisis de Valores Faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de valores faltantes\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (missing_data / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Variable': df.columns,\n",
    "    'Valores Faltantes': missing_data,\n",
    "    'Porcentaje': missing_percent\n",
    "})\n",
    "missing_df = missing_df[missing_df['Valores Faltantes'] > 0].sort_values('Porcentaje', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(\"=== VALORES FALTANTES ===\")\n",
    "    display(missing_df)\n",
    "    \n",
    "    # Visualización de valores faltantes\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(df.isnull(), cbar=True, xticklabels=True, yticklabels=False, cmap='viridis')\n",
    "    plt.title('Patrón de Valores Faltantes')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\nelse:\n",
    "    print(\"✅ No se encontraron valores faltantes en el dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Análisis de Distribuciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuciones de variables numéricas\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "n_cols = len(numeric_cols)\n",
    "n_rows = (n_cols + 2) // 3\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, 3, figsize=(18, 6*n_rows))\n",
    "axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
    "\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Histograma con curva de densidad\n",
    "    df[col].hist(bins=30, density=True, alpha=0.7, ax=ax, color='skyblue')\n",
    "    \n",
    "    # Curva de densidad\n",
    "    df[col].plot.density(ax=ax, color='red', linewidth=2)\n",
    "    \n",
    "    ax.set_title(f'Distribución de {col}')\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel('Densidad')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Ocultar axes vacíos\n",
    "for i in range(n_cols, len(axes)):\n",
    "    axes[i].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots para detectar outliers\n",
    "fig, axes = plt.subplots(n_rows, 3, figsize=(18, 6*n_rows))\n",
    "axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
    "\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    ax = axes[i]\n",
    "    df.boxplot(column=col, ax=ax)\n",
    "    ax.set_title(f'Box Plot de {col}')\n",
    "    ax.set_ylabel(col)\n",
    "\n",
    "# Ocultar axes vacíos\n",
    "for i in range(n_cols, len(axes)):\n",
    "    axes[i].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Análisis de Correlaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correlación\n",
    "correlation_matrix = df[numeric_cols].corr()\n",
    "\n",
    "# Heatmap de correlaciones\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, \n",
    "            mask=mask,\n",
    "            annot=True, \n",
    "            cmap='RdBu_r', \n",
    "            center=0,\n",
    "            square=True,\n",
    "            fmt='.2f',\n",
    "            cbar_kws={'shrink': 0.8})\n",
    "plt.title('Matriz de Correlación', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlación con la variable target\n",
    "if 'target' in df.columns:\n",
    "    target_correlations = df[numeric_cols].corrwith(df['target']).sort_values(key=abs, ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    colors = ['red' if x < 0 else 'green' for x in target_correlations.values]\n",
    "    target_correlations.plot(kind='barh', color=colors)\n",
    "    plt.title('Correlación de Variables con Target')\n",
    "    plt.xlabel('Correlación')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"=== CORRELACIONES CON TARGET ===\")\n",
    "    for var, corr in target_correlations.items():\n",
    "        if var != 'target':\n",
    "            print(f\"{var}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Detección de Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detección de outliers usando el método IQR\n",
    "def detect_outliers_iqr(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "outlier_summary = []\n",
    "for col in numeric_cols:\n",
    "    outliers, lower, upper = detect_outliers_iqr(df, col)\n",
    "    outlier_summary.append({\n",
    "        'Variable': col,\n",
    "        'Outliers': len(outliers),\n",
    "        'Porcentaje': (len(outliers) / len(df)) * 100,\n",
    "        'Límite Inferior': lower,\n",
    "        'Límite Superior': upper\n",
    "    })\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary)\n",
    "print(\"=== RESUMEN DE OUTLIERS (Método IQR) ===\")\n",
    "display(outlier_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Análisis de Normalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de normalidad Shapiro-Wilk (para muestras pequeñas) o Anderson-Darling\n",
    "normality_results = []\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if len(df[col].dropna()) <= 5000:  # Shapiro-Wilk para muestras pequeñas\n",
    "        stat, p_value = stats.shapiro(df[col].dropna())\n",
    "        test_name = 'Shapiro-Wilk'\n",
    "    else:  # Anderson-Darling para muestras grandes\n",
    "        result = stats.anderson(df[col].dropna())\n",
    "        stat = result.statistic\n",
    "        p_value = 0.05 if stat > result.critical_values[2] else 0.1  # Aproximación\n",
    "        test_name = 'Anderson-Darling'\n",
    "    \n",
    "    is_normal = p_value > 0.05\n",
    "    normality_results.append({\n",
    "        'Variable': col,\n",
    "        'Test': test_name,\n",
    "        'Estadístico': stat,\n",
    "        'p-valor': p_value,\n",
    "        'Normal': 'Sí' if is_normal else 'No'\n",
    "    })\n",
    "\n",
    "normality_df = pd.DataFrame(normality_results)\n",
    "print(\"=== TESTS DE NORMALIDAD ===\")\n",
    "display(normality_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Scatter Plots con Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots de variables vs target\n",
    "if 'target' in df.columns:\n",
    "    feature_cols = [col for col in numeric_cols if col != 'target']\n",
    "    n_features = len(feature_cols)\n",
    "    n_rows = (n_features + 2) // 3\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, 3, figsize=(18, 6*n_rows))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_features == 1 else axes\n",
    "    \n",
    "    for i, col in enumerate(feature_cols):\n",
    "        ax = axes[i]\n",
    "        ax.scatter(df[col], df['target'], alpha=0.6, s=20)\n",
    "        \n",
    "        # Línea de tendencia\n",
    "        z = np.polyfit(df[col], df['target'], 1)\n",
    "        p = np.poly1d(z)\n",
    "        ax.plot(df[col], p(df[col]), \"r--\", alpha=0.8, linewidth=2)\n",
    "        \n",
    "        ax.set_xlabel(col)\n",
    "        ax.set_ylabel('Target')\n",
    "        ax.set_title(f'{col} vs Target')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Ocultar axes vacíos\n",
    "    for i in range(n_features, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Análisis de Feature Importance Preliminar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance usando correlación absoluta y mutual information\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "if 'target' in df.columns:\n",
    "    feature_cols = [col for col in numeric_cols if col != 'target']\n",
    "    X = df[feature_cols]\n",
    "    y = df['target']\n",
    "    \n",
    "    # Remover filas con valores faltantes\n",
    "    mask = ~(X.isnull().any(axis=1) | y.isnull())\n",
    "    X_clean = X[mask]\n",
    "    y_clean = y[mask]\n",
    "    \n",
    "    # Normalizar para mutual information\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_clean)\n",
    "    \n",
    "    # Calcular mutual information\n",
    "    mi_scores = mutual_info_regression(X_scaled, y_clean)\n",
    "    \n",
    "    # Crear DataFrame con resultados\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Variable': feature_cols,\n",
    "        'Correlación_Abs': [abs(df[col].corr(df['target'])) for col in feature_cols],\n",
    "        'Mutual_Information': mi_scores\n",
    "    })\n",
    "    \n",
    "    # Normalizar mutual information para comparación\n",
    "    importance_df['MI_Normalizada'] = importance_df['Mutual_Information'] / importance_df['Mutual_Information'].max()\n",
    "    \n",
    "    importance_df = importance_df.sort_values('MI_Normalizada', ascending=False)\n",
    "    \n",
    "    print(\"=== IMPORTANCIA PRELIMINAR DE CARACTERÍSTICAS ===\")\n",
    "    display(importance_df)\n",
    "    \n",
    "    # Visualización\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Correlación absoluta\n",
    "    importance_df.set_index('Variable')['Correlación_Abs'].plot(kind='barh', ax=ax1, color='lightblue')\n",
    "    ax1.set_title('Importancia por Correlación Absoluta')\n",
    "    ax1.set_xlabel('Correlación Absoluta')\n",
    "    \n",
    "    # Mutual Information\n",
    "    importance_df.set_index('Variable')['MI_Normalizada'].plot(kind='barh', ax=ax2, color='lightgreen')\n",
    "    ax2.set_title('Importancia por Mutual Information')\n",
    "    ax2.set_xlabel('MI Normalizada')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Resumen y Conclusiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== RESUMEN DEL ANÁLISIS EXPLORATORIO ===\")\n",
    "print(f\"\\n📊 INFORMACIÓN GENERAL:\")\n",
    "print(f\"   • Dataset: {df.shape[0]} filas x {df.shape[1]} columnas\")\n",
    "print(f\"   • Variables numéricas: {len(numeric_cols)}\")\n",
    "print(f\"   • Memoria utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(f\"\\n❌ VALORES FALTANTES:\")\n",
    "    for _, row in missing_df.iterrows():\n",
    "        print(f\"   • {row['Variable']}: {row['Valores Faltantes']} ({row['Porcentaje']:.1f}%)\")\nelse:\n",
    "    print(f\"\\n✅ VALORES FALTANTES: Ninguno\")\n",
    "\n",
    "print(f\"\\n🎯 OUTLIERS DETECTADOS:\")\n",
    "for _, row in outlier_df.iterrows():\n",
    "    if row['Outliers'] > 0:\n",
    "        print(f\"   • {row['Variable']}: {row['Outliers']} ({row['Porcentaje']:.1f}%)\")\n",
    "\n",
    "if 'target' in df.columns:\n",
    "    print(f\"\\n🔗 CORRELACIONES MÁS FUERTES CON TARGET:\")\n",
    "    top_corrs = target_correlations.drop('target').head(3)\n",
    "    for var, corr in top_corrs.items():\n",
    "        print(f\"   • {var}: {corr:.3f}\")\n",
    "\n",
    "print(f\"\\n📈 DISTRIBUCIONES:\")\n",
    "normal_vars = normality_df[normality_df['Normal'] == 'Sí']['Variable'].tolist()\n",
    "if normal_vars:\n",
    "    print(f\"   • Variables con distribución normal: {', '.join(normal_vars)}\")\nelse:\n",
    "    print(f\"   • Ninguna variable sigue distribución normal\")\n",
    "\n",
    "print(f\"\\n🚀 RECOMENDACIONES PARA MODELADO:\")\n",
    "print(f\"   • Considerar transformaciones para variables no normales\")\n",
    "print(f\"   • Evaluar el tratamiento de outliers\")\n",
    "print(f\"   • Usar feature scaling para algoritmos sensibles a escala\")\n",
    "print(f\"   • Considerar feature engineering basado en correlaciones\")\n",
    "if len(missing_df) > 0:\n",
    "    print(f\"   • Implementar estrategia de imputación para valores faltantes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}