{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Model Training and Evaluation\n",
    "This notebook contains training and evaluation of machine learning models for regression.\n",
    "\n",
    "## Models to evaluate:\n",
    "1. **Multivariate Linear Regression** (LinearRegression)\n",
    "2. **Support Vector Regression (SVR)**\n",
    "3. **XGBoost Regressor**\n",
    "\n",
    "## Objectives:\n",
    "- Train models with different hyperparameters\n",
    "- Evaluate performance with regression metrics\n",
    "- Analyze feature importance\n",
    "- Compare models and select the best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, validation_curve\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "import xgboost as xgb\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the dataset\n# NOTE: Replace 'your_dataset.csv' with the actual path to your file\nfile_path = 'your_dataset.csv'\n\ntry:\n    df = pd.read_csv(file_path)\n    print(f\"Dataset loaded successfully: {df.shape[0]} rows and {df.shape[1]} columns\")\nexcept FileNotFoundError:\n    print(\"File not found. Using example dataset.\")\n    # Create example dataset for demonstration with 10 features\n    np.random.seed(42)\n    n_samples = 1000\n    df = pd.DataFrame({\n        'feature_1': np.random.normal(50, 15, n_samples),\n        'feature_2': np.random.exponential(2, n_samples),\n        'feature_3': np.random.uniform(0, 100, n_samples),\n        'feature_4': np.random.gamma(2, 2, n_samples),\n        'feature_5': np.random.beta(2, 5, n_samples) * 100,\n        'feature_6': np.random.lognormal(1, 0.5, n_samples),\n        'feature_7': np.random.weibull(1.5, n_samples) * 50,\n        'feature_8': np.random.poisson(5, n_samples),\n        'feature_9': np.random.triangular(0, 50, 100, n_samples),\n        'feature_10': np.random.pareto(3, n_samples) * 10,\n        'target': np.random.normal(75, 20, n_samples)\n    })\n    # Add artificial correlation with multiple features\n    df['target'] = (0.25 * df['feature_1'] + 0.15 * df['feature_3'] + 0.1 * df['feature_4'] + \n                   0.08 * df['feature_6'] + 0.12 * df['feature_7'] + 0.05 * df['feature_9'] + \n                   np.random.normal(0, 10, n_samples))\n    print(f\"Example dataset created: {df.shape[0]} rows and {df.shape[1]} columns\")\n\n# Check if target column exists\nif 'target' not in df.columns:\n    print(\"‚ö†Ô∏è WARNING: 'target' column not found. Please specify your target variable.\")\n    print(f\"Available columns: {list(df.columns)}\")\nelse:\n    print(f\"‚úÖ Target variable 'target' found\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "# Separate features and target variable\n",
    "feature_cols = [col for col in df.columns if col != 'target']\n",
    "X = df[feature_cols]\n",
    "y = df['target']\n",
    "\n",
    "print(f\"Selected features: {feature_cols}\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_X = X.isnull().sum().sum()\n",
    "missing_y = y.isnull().sum()\n",
    "print(f\"\\nMissing values in X: {missing_X}\")\n",
    "print(f\"Missing values in y: {missing_y}\")\n",
    "\n",
    "# Remove rows with missing values if they exist\n",
    "if missing_X > 0 or missing_y > 0:\n",
    "    mask = ~(X.isnull().any(axis=1) | y.isnull())\n",
    "    X = X[mask]\n",
    "    y = y[mask]\n",
    "    print(f\"After cleaning data: X={X.shape}, y={y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=None\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Train/test ratio: {X_train.shape[0]/X_test.shape[0]:.1f}\")\n",
    "\n",
    "# Target variable statistics\n",
    "print(f\"\\nTarget variable statistics:\")\n",
    "print(f\"Train - Mean: {y_train.mean():.2f}, Std: {y_train.std():.2f}\")\n",
    "print(f\"Test  - Mean: {y_test.mean():.2f}, Std: {y_test.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare different scalers\n",
    "scalers = {\n",
    "    'StandardScaler': StandardScaler(),\n",
    "    'RobustScaler': RobustScaler(),\n",
    "    'NoScaler': None\n",
    "}\n",
    "\n",
    "# Function to apply scaling\n",
    "def apply_scaling(scaler, X_train, X_test):\n",
    "    if scaler is None:\n",
    "        return X_train, X_test\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "print(\"Scalers prepared:\")\n",
    "for name in scalers.keys():\n",
    "    print(f\"  ‚Ä¢ {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation Metrics Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate multiple regression metrics\"\"\"\n",
    "    metrics = {\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'MAE': mean_absolute_error(y_true, y_pred),\n",
    "        'R¬≤': r2_score(y_true, y_pred),\n",
    "        'MAPE': mean_absolute_percentage_error(y_true, y_pred) * 100,\n",
    "        'Adjusted_R¬≤': 1 - (1 - r2_score(y_true, y_pred)) * (len(y_true) - 1) / (len(y_true) - X_train.shape[1] - 1)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def print_metrics(metrics, title=\"Metrics\"):\n",
    "    \"\"\"Print metrics in organized format\"\"\"\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    for metric, value in metrics.items():\n",
    "        if metric in ['R¬≤', 'Adjusted_R¬≤']:\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "        elif metric == 'MAPE':\n",
    "            print(f\"{metric}: {value:.2f}%\")\n",
    "        else:\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(\"Metrics functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model 1: Linear Regression with Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç TRAINING LINEAR REGRESSION MODELS\")\n",
    "\n",
    "# Linear regression models to test\n",
    "linear_models = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'Ridge': Ridge(),\n",
    "    'Lasso': Lasso(max_iter=2000),\n",
    "    'ElasticNet': ElasticNet(max_iter=2000)\n",
    "}\n",
    "\n",
    "# Hyperparameters for search\n",
    "linear_params = {\n",
    "    'Ridge': {'alpha': [0.1, 1, 10, 100, 1000]},\n",
    "    'Lasso': {'alpha': [0.001, 0.01, 0.1, 1, 10]},\n",
    "    'ElasticNet': {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1],\n",
    "        'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Linear model results\n",
    "linear_results = {}\n",
    "\n",
    "# Use StandardScaler for linear models\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled, X_test_scaled = apply_scaling(scaler, X_train, X_test)\n",
    "\n",
    "for model_name, model in linear_models.items():\n",
    "    print(f\"\\nüîß Training {model_name}...\")\n",
    "    \n",
    "    if model_name in linear_params:\n",
    "        # Hyperparameter search with cross-validation\n",
    "        grid_search = GridSearchCV(\n",
    "            model, linear_params[model_name], \n",
    "            cv=5, scoring='neg_mean_squared_error', \n",
    "            n_jobs=-1\n",
    "        )\n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        print(f\"  Best hyperparameters: {grid_search.best_params_}\")\n",
    "    else:\n",
    "        # Model without hyperparameters\n",
    "        best_model = model\n",
    "        best_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = best_model.predict(X_train_scaled)\n",
    "    y_test_pred = best_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Metrics\n",
    "    train_metrics = calculate_metrics(y_train, y_train_pred)\n",
    "    test_metrics = calculate_metrics(y_test, y_test_pred)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(best_model, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    cv_rmse = np.sqrt(-cv_scores.mean())\n",
    "    cv_rmse_std = np.sqrt(cv_scores.std())\n",
    "    \n",
    "    linear_results[model_name] = {\n",
    "        'model': best_model,\n",
    "        'train_metrics': train_metrics,\n",
    "        'test_metrics': test_metrics,\n",
    "        'cv_rmse': cv_rmse,\n",
    "        'cv_rmse_std': cv_rmse_std,\n",
    "        'predictions': {'train': y_train_pred, 'test': y_test_pred}\n",
    "    }\n",
    "    \n",
    "    print_metrics(train_metrics, \"Training\")\n",
    "    print_metrics(test_metrics, \"Test\")\n",
    "    print(f\"CV RMSE: {cv_rmse:.4f} (¬±{cv_rmse_std:.4f})\")\n",
    "\n",
    "print(\"\\n‚úÖ Linear models trained successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model 2: Support Vector Regression (SVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç TRAINING SUPPORT VECTOR REGRESSION\")\n",
    "\n",
    "# Hyperparameters for SVR\n",
    "svr_params = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "    'epsilon': [0.01, 0.1, 0.2, 0.5]\n",
    "}\n",
    "\n",
    "# Different kernels to test\n",
    "svr_kernels = ['linear', 'rbf', 'poly']\n",
    "svr_results = {}\n",
    "\n",
    "# Use RobustScaler for SVR (more robust to outliers)\n",
    "robust_scaler = RobustScaler()\n",
    "X_train_robust, X_test_robust = apply_scaling(robust_scaler, X_train, X_test)\n",
    "\n",
    "for kernel in svr_kernels:\n",
    "    print(f\"\\nüîß Training SVR with {kernel} kernel...\")\n",
    "    \n",
    "    # Create SVR model\n",
    "    svr_model = SVR(kernel=kernel)\n",
    "    \n",
    "    # Adjust parameters according to kernel\n",
    "    current_params = svr_params.copy()\n",
    "    if kernel == 'linear':\n",
    "        current_params.pop('gamma')  # Gamma not relevant for linear kernel\n",
    "    elif kernel == 'poly':\n",
    "        current_params['degree'] = [2, 3, 4]  # Add degree for polynomial\n",
    "    \n",
    "    # Hyperparameter search\n",
    "    grid_search = GridSearchCV(\n",
    "        svr_model, current_params, \n",
    "        cv=5, scoring='neg_mean_squared_error', \n",
    "        n_jobs=-1, verbose=0\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train_robust, y_train)\n",
    "    best_svr = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"  Best hyperparameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = best_svr.predict(X_train_robust)\n",
    "    y_test_pred = best_svr.predict(X_test_robust)\n",
    "    \n",
    "    # Metrics\n",
    "    train_metrics = calculate_metrics(y_train, y_train_pred)\n",
    "    test_metrics = calculate_metrics(y_test, y_test_pred)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(best_svr, X_train_robust, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    cv_rmse = np.sqrt(-cv_scores.mean())\n",
    "    cv_rmse_std = np.sqrt(cv_scores.std())\n",
    "    \n",
    "    svr_results[f'SVR_{kernel}'] = {\n",
    "        'model': best_svr,\n",
    "        'train_metrics': train_metrics,\n",
    "        'test_metrics': test_metrics,\n",
    "        'cv_rmse': cv_rmse,\n",
    "        'cv_rmse_std': cv_rmse_std,\n",
    "        'predictions': {'train': y_train_pred, 'test': y_test_pred}\n",
    "    }\n",
    "    \n",
    "    print_metrics(train_metrics, \"Training\")\n",
    "    print_metrics(test_metrics, \"Test\")\n",
    "    print(f\"CV RMSE: {cv_rmse:.4f} (¬±{cv_rmse_std:.4f})\")\n",
    "\n",
    "print(\"\\n‚úÖ SVR models trained successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model 3: XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç TRAINING XGBOOST REGRESSOR\")\n",
    "\n",
    "# Hyperparameters for XGBoost\n",
    "xgb_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Create XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    random_state=42,\n",
    "    verbosity=0,\n",
    "    eval_metric='rmse'\n",
    ")\n",
    "\n",
    "# XGBoost doesn't require scaling, use original data\n",
    "print(\"üîß Training XGBoost (may take several minutes)...\")\n",
    "\n",
    "# Random search to reduce computation time\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Use RandomizedSearchCV instead of GridSearchCV for efficiency\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgb_model, xgb_params,\n",
    "    n_iter=50,  # Number of combinations to test\n",
    "    cv=5, scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1, random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "best_xgb = random_search.best_estimator_\n",
    "\n",
    "print(f\"  Best hyperparameters: {random_search.best_params_}\")\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = best_xgb.predict(X_train)\n",
    "y_test_pred = best_xgb.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "train_metrics = calculate_metrics(y_train, y_train_pred)\n",
    "test_metrics = calculate_metrics(y_test, y_test_pred)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(best_xgb, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_rmse = np.sqrt(-cv_scores.mean())\n",
    "cv_rmse_std = np.sqrt(cv_scores.std())\n",
    "\n",
    "xgb_results = {\n",
    "    'model': best_xgb,\n",
    "    'train_metrics': train_metrics,\n",
    "    'test_metrics': test_metrics,\n",
    "    'cv_rmse': cv_rmse,\n",
    "    'cv_rmse_std': cv_rmse_std,\n",
    "    'predictions': {'train': y_train_pred, 'test': y_test_pred}\n",
    "}\n",
    "\n",
    "print_metrics(train_metrics, \"Training\")\n",
    "print_metrics(test_metrics, \"Test\")\n",
    "print(f\"CV RMSE: {cv_rmse:.4f} (¬±{cv_rmse_std:.4f})\")\n",
    "\n",
    "print(\"\\n‚úÖ XGBoost model trained successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results\n",
    "all_results = {**linear_results, **svr_results, 'XGBoost': xgb_results}\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "for model_name, results in all_results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Train_RMSE': results['train_metrics']['RMSE'],\n",
    "        'Test_RMSE': results['test_metrics']['RMSE'],\n",
    "        'Train_R¬≤': results['train_metrics']['R¬≤'],\n",
    "        'Test_R¬≤': results['test_metrics']['R¬≤'],\n",
    "        'Train_MAE': results['train_metrics']['MAE'],\n",
    "        'Test_MAE': results['test_metrics']['MAE'],\n",
    "        'CV_RMSE': results['cv_rmse'],\n",
    "        'CV_RMSE_Std': results['cv_rmse_std'],\n",
    "        'Overfitting': results['train_metrics']['RMSE'] - results['test_metrics']['RMSE']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('Test_R¬≤', ascending=False)\n",
    "\n",
    "print(\"=== MODEL COMPARISON ===\")\n",
    "display(comparison_df.round(4))\n",
    "\n",
    "# Identify best model\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
    "print(f\"   Test R¬≤: {comparison_df.iloc[0]['Test_R¬≤']:.4f}\")\n",
    "print(f\"   Test RMSE: {comparison_df.iloc[0]['Test_RMSE']:.4f}\")\n",
    "print(f\"   CV RMSE: {comparison_df.iloc[0]['CV_RMSE']:.4f} (¬±{comparison_df.iloc[0]['CV_RMSE_Std']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# R¬≤ Score\n",
    "ax1 = axes[0, 0]\n",
    "x_pos = np.arange(len(comparison_df))\n",
    "ax1.bar(x_pos, comparison_df['Test_R¬≤'], alpha=0.7, color='lightblue')\n",
    "ax1.set_xlabel('Models')\n",
    "ax1.set_ylabel('R¬≤ Score')\n",
    "ax1.set_title('R¬≤ Score on Test Set')\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(comparison_df['Model'], rotation=45)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# RMSE\n",
    "ax2 = axes[0, 1]\n",
    "ax2.bar(x_pos, comparison_df['Test_RMSE'], alpha=0.7, color='lightcoral')\n",
    "ax2.set_xlabel('Models')\n",
    "ax2.set_ylabel('RMSE')\n",
    "ax2.set_title('RMSE on Test Set')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(comparison_df['Model'], rotation=45)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Overfitting (Train RMSE - Test RMSE)\n",
    "ax3 = axes[1, 0]\n",
    "colors = ['red' if x > 0 else 'green' for x in comparison_df['Overfitting']]\n",
    "ax3.bar(x_pos, comparison_df['Overfitting'], alpha=0.7, color=colors)\n",
    "ax3.set_xlabel('Models')\n",
    "ax3.set_ylabel('Overfitting (Train RMSE - Test RMSE)')\n",
    "ax3.set_title('Overfitting Analysis')\n",
    "ax3.set_xticks(x_pos)\n",
    "ax3.set_xticklabels(comparison_df['Model'], rotation=45)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Cross-Validation RMSE with error bars\n",
    "ax4 = axes[1, 1]\n",
    "ax4.bar(x_pos, comparison_df['CV_RMSE'], alpha=0.7, color='lightgreen',\n",
    "        yerr=comparison_df['CV_RMSE_Std'], capsize=5)\n",
    "ax4.set_xlabel('Models')\n",
    "ax4.set_ylabel('CV RMSE')\n",
    "ax4.set_title('Cross-Validation RMSE')\n",
    "ax4.set_xticks(x_pos)\n",
    "ax4.set_xticklabels(comparison_df['Model'], rotation=45)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç ANALYZING FEATURE IMPORTANCE\")\n",
    "\n",
    "# Function to get feature importance\n",
    "def get_feature_importance(model, model_name, X_test, y_test):\n",
    "    \"\"\"Get feature importance according to model type\"\"\"\n",
    "    \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        # Models with feature_importances_ (XGBoost, RandomForest, etc.)\n",
    "        importance = model.feature_importances_\n",
    "        method = 'Built-in Feature Importance'\n",
    "    \n",
    "    elif hasattr(model, 'coef_'):\n",
    "        # Linear models\n",
    "        importance = np.abs(model.coef_)\n",
    "        method = 'Absolute Coefficients'\n",
    "    \n",
    "    else:\n",
    "        # Use permutation importance for other models\n",
    "        perm_importance = permutation_importance(\n",
    "            model, X_test, y_test, \n",
    "            n_repeats=10, random_state=42, \n",
    "            scoring='neg_mean_squared_error'\n",
    "        )\n",
    "        importance = perm_importance.importances_mean\n",
    "        method = 'Permutation Importance'\n",
    "    \n",
    "    return importance, method\n",
    "\n",
    "# Analyze importance for top 3 models\n",
    "top_3_models = comparison_df.head(3)\n",
    "importance_results = {}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for i, (_, row) in enumerate(top_3_models.iterrows()):\n",
    "    model_name = row['Model']\n",
    "    model_obj = all_results[model_name]['model']\n",
    "    \n",
    "    # Prepare test data according to model\n",
    "    if model_name.startswith('SVR'):\n",
    "        X_test_prepared = robust_scaler.transform(X_test)\n",
    "    elif model_name in ['LinearRegression', 'Ridge', 'Lasso', 'ElasticNet']:\n",
    "        X_test_prepared = scaler.transform(X_test)\n",
    "    else:  # XGBoost\n",
    "        X_test_prepared = X_test\n",
    "    \n",
    "    importance, method = get_feature_importance(model_obj, model_name, X_test_prepared, y_test)\n",
    "    \n",
    "    # Normalize importance\n",
    "    importance_normalized = importance / importance.sum()\n",
    "    \n",
    "    # Create DataFrame for visualization\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_cols,\n",
    "        'Importance': importance_normalized\n",
    "    }).sort_values('Importance', ascending=True)\n",
    "    \n",
    "    importance_results[model_name] = importance_df\n",
    "    \n",
    "    # Plot\n",
    "    ax = axes[i]\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(importance_df)))\n",
    "    bars = ax.barh(importance_df['Feature'], importance_df['Importance'], color=colors)\n",
    "    ax.set_title(f'{model_name}\\n({method})')\n",
    "    ax.set_xlabel('Normalized Importance')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add values on bars\n",
    "    for bar, importance in zip(bars, importance_df['Importance']):\n",
    "        ax.text(bar.get_width() + 0.005, bar.get_y() + bar.get_height()/2, \n",
    "                f'{importance:.3f}', ha='left', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print feature importance ranking\n",
    "print(\"\\n=== FEATURE IMPORTANCE RANKING ===\")\n",
    "for model_name, imp_df in importance_results.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    for i, (_, row) in enumerate(imp_df.sort_values('Importance', ascending=False).iterrows(), 1):\n",
    "        print(f\"  {i}. {row['Feature']}: {row['Importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Residual Analysis of Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model for detailed analysis\n",
    "best_model_results = all_results[best_model_name]\n",
    "best_model = best_model_results['model']\n",
    "y_train_pred = best_model_results['predictions']['train']\n",
    "y_test_pred = best_model_results['predictions']['test']\n",
    "\n",
    "print(f\"üîç RESIDUAL ANALYSIS - {best_model_name}\")\n",
    "\n",
    "# Calculate residuals\n",
    "train_residuals = y_train - y_train_pred\n",
    "test_residuals = y_test - y_test_pred\n",
    "\n",
    "# Create residual analysis plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Predictions vs Actual Values (Train)\n",
    "ax1 = axes[0, 0]\n",
    "ax1.scatter(y_train, y_train_pred, alpha=0.6, s=20)\n",
    "min_val = min(y_train.min(), y_train_pred.min())\n",
    "max_val = max(y_train.max(), y_train_pred.max())\n",
    "ax1.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
    "ax1.set_xlabel('Actual Values')\n",
    "ax1.set_ylabel('Predictions')\n",
    "ax1.set_title('Train: Predictions vs Actual')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Predictions vs Actual Values (Test)\n",
    "ax2 = axes[0, 1]\n",
    "ax2.scatter(y_test, y_test_pred, alpha=0.6, s=20, color='orange')\n",
    "min_val = min(y_test.min(), y_test_pred.min())\n",
    "max_val = max(y_test.max(), y_test_pred.max())\n",
    "ax2.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
    "ax2.set_xlabel('Actual Values')\n",
    "ax2.set_ylabel('Predictions')\n",
    "ax2.set_title('Test: Predictions vs Actual')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Residuals vs Predictions (Test)\n",
    "ax3 = axes[0, 2]\n",
    "ax3.scatter(y_test_pred, test_residuals, alpha=0.6, s=20, color='green')\n",
    "ax3.axhline(y=0, color='r', linestyle='--')\n",
    "ax3.set_xlabel('Predictions')\n",
    "ax3.set_ylabel('Residuals')\n",
    "ax3.set_title('Residuals vs Predictions (Test)')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Histogram of Residuals (Train)\n",
    "ax4 = axes[1, 0]\n",
    "ax4.hist(train_residuals, bins=30, alpha=0.7, density=True, color='skyblue')\n",
    "ax4.axvline(train_residuals.mean(), color='red', linestyle='--', label=f'Mean: {train_residuals.mean():.3f}')\n",
    "ax4.set_xlabel('Residuals')\n",
    "ax4.set_ylabel('Density')\n",
    "ax4.set_title('Residual Distribution (Train)')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Histogram of Residuals (Test)\n",
    "ax5 = axes[1, 1]\n",
    "ax5.hist(test_residuals, bins=30, alpha=0.7, density=True, color='orange')\n",
    "ax5.axvline(test_residuals.mean(), color='red', linestyle='--', label=f'Mean: {test_residuals.mean():.3f}')\n",
    "ax5.set_xlabel('Residuals')\n",
    "ax5.set_ylabel('Density')\n",
    "ax5.set_title('Residual Distribution (Test)')\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Q-Q Plot of Residuals (Test)\n",
    "ax6 = axes[1, 2]\n",
    "stats.probplot(test_residuals, dist=\"norm\", plot=ax6)\n",
    "ax6.set_title('Q-Q Plot of Residuals (Test)')\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Residual statistics\n",
    "print(f\"\\n=== RESIDUAL STATISTICS ===\")\n",
    "print(f\"Train:\")\n",
    "print(f\"  Mean: {train_residuals.mean():.6f}\")\n",
    "print(f\"  Std: {train_residuals.std():.4f}\")\n",
    "print(f\"  Skewness: {stats.skew(train_residuals):.4f}\")\n",
    "print(f\"  Kurtosis: {stats.kurtosis(train_residuals):.4f}\")\n",
    "\n",
    "print(f\"\\nTest:\")\n",
    "print(f\"  Mean: {test_residuals.mean():.6f}\")\n",
    "print(f\"  Std: {test_residuals.std():.4f}\")\n",
    "print(f\"  Skewness: {stats.skew(test_residuals):.4f}\")\n",
    "print(f\"  Kurtosis: {stats.kurtosis(test_residuals):.4f}\")\n",
    "\n",
    "# Normality test for residuals\n",
    "_, p_value = stats.shapiro(test_residuals[:1000] if len(test_residuals) > 1000 else test_residuals)\n",
    "print(f\"\\nNormality Test (Shapiro-Wilk): p-value = {p_value:.6f}\")\n",
    "print(f\"Residuals are normal: {'Yes' if p_value > 0.05 else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Learning and Validation Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "print(f\"üîç GENERATING LEARNING CURVES FOR {best_model_name}\")\n",
    "\n",
    "# Prepare data according to best model\n",
    "if best_model_name.startswith('SVR'):\n",
    "    X_for_curves = robust_scaler.fit_transform(X_train)\n",
    "elif best_model_name in ['LinearRegression', 'Ridge', 'Lasso', 'ElasticNet']:\n",
    "    X_for_curves = scaler.fit_transform(X_train)\n",
    "else:  # XGBoost\n",
    "    X_for_curves = X_train\n",
    "\n",
    "# Generate learning curves\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_model, X_for_curves, y_train,\n",
    "    cv=5, n_jobs=-1,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "# Convert to RMSE\n",
    "train_rmse = np.sqrt(-train_scores)\n",
    "val_rmse = np.sqrt(-val_scores)\n",
    "\n",
    "# Calculate means and standard deviations\n",
    "train_rmse_mean = train_rmse.mean(axis=1)\n",
    "train_rmse_std = train_rmse.std(axis=1)\n",
    "val_rmse_mean = val_rmse.mean(axis=1)\n",
    "val_rmse_std = val_rmse.std(axis=1)\n",
    "\n",
    "# Learning curves plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(train_sizes, train_rmse_mean, 'o-', color='blue', label='Training RMSE')\n",
    "plt.fill_between(train_sizes, train_rmse_mean - train_rmse_std, \n",
    "                 train_rmse_mean + train_rmse_std, alpha=0.1, color='blue')\n",
    "\n",
    "plt.plot(train_sizes, val_rmse_mean, 'o-', color='red', label='Validation RMSE')\n",
    "plt.fill_between(train_sizes, val_rmse_mean - val_rmse_std, \n",
    "                 val_rmse_mean + val_rmse_std, alpha=0.1, color='red')\n",
    "\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title(f'Learning Curves - {best_model_name}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Learning curves analysis\n",
    "final_gap = val_rmse_mean[-1] - train_rmse_mean[-1]\n",
    "print(f\"\\n=== LEARNING CURVES ANALYSIS ===\")\n",
    "print(f\"Final training RMSE: {train_rmse_mean[-1]:.4f} (¬±{train_rmse_std[-1]:.4f})\")\n",
    "print(f\"Final validation RMSE: {val_rmse_mean[-1]:.4f} (¬±{val_rmse_std[-1]:.4f})\")\n",
    "print(f\"Final gap (Val - Train): {final_gap:.4f}\")\n",
    "\n",
    "if final_gap > 0.1 * train_rmse_mean[-1]:\n",
    "    print(\"‚ö†Ô∏è Possible overfitting detected\")\nelif final_gap < 0:\n",
    "    print(\"‚ö†Ô∏è Possible underfitting or easier validation data\")\nelse:\n",
    "    print(\"‚úÖ Good balance between bias and variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Weight Optimization to Maximize Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize, differential_evolution\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print(\"üéØ WEIGHT OPTIMIZATION TO MAXIMIZE OUTPUT\")\n",
    "\n",
    "# Objective function to maximize model prediction\n",
    "def objective_function(weights, model, scaler=None, feature_names=None):\n",
    "    \"\"\"Objective function: we want to maximize model output\"\"\"\n",
    "    weights_reshaped = weights.reshape(1, -1)\n",
    "    \n",
    "    # Apply scaling if necessary\n",
    "    if scaler is not None:\n",
    "        weights_scaled = scaler.transform(weights_reshaped)\n",
    "    else:\n",
    "        weights_scaled = weights_reshaped\n",
    "    \n",
    "    # Predict (we want to maximize, so return negative)\n",
    "    prediction = model.predict(weights_scaled)[0]\n",
    "    return -prediction  # Negative because minimize searches for minimum\n",
    "\n",
    "# Define feature bounds based on data\n",
    "feature_bounds = []\n",
    "print(\"\\nFeature bounds based on data:\")\n",
    "for i, feature in enumerate(feature_cols):\n",
    "    min_val = X[feature].min()\n",
    "    max_val = X[feature].max()\n",
    "    feature_bounds.append((min_val, max_val))\n",
    "    print(f\"  {feature}: [{min_val:.2f}, {max_val:.2f}]\")\n",
    "\n",
    "# Optimize for best model\n",
    "print(f\"\\nüîß Optimizing with {best_model_name}...\")\n",
    "\n",
    "# Prepare scaler if necessary\n",
    "optimization_scaler = None\n",
    "if best_model_name.startswith('SVR'):\n",
    "    optimization_scaler = RobustScaler().fit(X_train)\nelif best_model_name in ['LinearRegression', 'Ridge', 'Lasso', 'ElasticNet']:\n",
    "    optimization_scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "# Use differential evolution (more robust for non-convex problems)\n",
    "result = differential_evolution(\n",
    "    objective_function,\n",
    "    feature_bounds,\n",
    "    args=(best_model, optimization_scaler, feature_cols),\n",
    "    seed=42,\n",
    "    maxiter=1000,\n",
    "    popsize=15\n",
    ")\n",
    "\n",
    "optimal_weights = result.x\n",
    "optimal_prediction = -result.fun  # Convert back (remove negative)\n",
    "\n",
    "print(f\"\\n‚úÖ OPTIMIZATION COMPLETED\")\n",
    "print(f\"Maximum predicted value: {optimal_prediction:.4f}\")\n",
    "print(f\"Number of evaluations: {result.nfev}\")\n",
    "print(f\"Success: {result.success}\")\n",
    "\n",
    "# Show optimal weights\n",
    "print(f\"\\n=== OPTIMAL WEIGHTS TO MAXIMIZE OUTPUT ===\")\n",
    "optimal_df = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Optimal_Value': optimal_weights,\n",
    "    'Data_Min': [X[col].min() for col in feature_cols],\n",
    "    'Data_Max': [X[col].max() for col in feature_cols],\n",
    "    'Percentile_in_Data': [stats.percentileofscore(X[col], val) for col, val in zip(feature_cols, optimal_weights)]\n",
    "})\n",
    "\n",
    "display(optimal_df.round(4))\n",
    "\n",
    "# Compare with existing dataset examples\n",
    "print(f\"\\n=== COMPARISON WITH EXISTING DATA ===\")\n",
    "# Find top 5 samples with highest target values\n",
    "top_5_indices = y.nlargest(5).index\n",
    "top_5_predictions = []\n",
    "\n",
    "for idx in top_5_indices:\n",
    "    sample = X.loc[idx].values.reshape(1, -1)\n",
    "    if optimization_scaler is not None:\n",
    "        sample_scaled = optimization_scaler.transform(sample)\n",
    "    else:\n",
    "        sample_scaled = sample\n",
    "    pred = best_model.predict(sample_scaled)[0]\n",
    "    top_5_predictions.append(pred)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Type': ['Dataset Top 1', 'Dataset Top 2', 'Dataset Top 3', 'Dataset Top 4', 'Dataset Top 5', 'OPTIMAL WEIGHTS'],\n",
    "    'Prediction': top_5_predictions + [optimal_prediction],\n",
    "    'Actual_Target': list(y.loc[top_5_indices]) + ['N/A']\n",
    "})\n",
    "\n",
    "display(comparison_df.round(4))\n",
    "\n",
    "improvement = optimal_prediction - max(top_5_predictions)\n",
    "print(f\"\\nImprovement over best dataset case: {improvement:.4f} ({improvement/max(top_5_predictions)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Optimal Weights Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal weights visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Optimal values vs data ranges\n",
    "ax1 = axes[0, 0]\n",
    "x_pos = np.arange(len(feature_cols))\n",
    "ax1.bar(x_pos, optimal_weights, alpha=0.7, color='gold', label='Optimal Values')\n",
    "\n",
    "# Add lines for min/max of data\n",
    "mins = [X[col].min() for col in feature_cols]\n",
    "maxs = [X[col].max() for col in feature_cols]\n",
    "ax1.errorbar(x_pos, optimal_weights, \n",
    "            yerr=[np.array(optimal_weights) - np.array(mins), \n",
    "                  np.array(maxs) - np.array(optimal_weights)], \n",
    "            fmt='none', color='red', alpha=0.5, capsize=5)\n",
    "\n",
    "ax1.set_xlabel('Features')\n",
    "ax1.set_ylabel('Values')\n",
    "ax1.set_title('Optimal Values vs Data Ranges')\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(feature_cols, rotation=45)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Percentiles of optimal values\n",
    "ax2 = axes[0, 1]\n",
    "percentiles = optimal_df['Percentile_in_Data']\n",
    "colors = ['red' if p > 90 else 'orange' if p > 75 else 'yellow' if p > 50 else 'lightblue' for p in percentiles]\n",
    "bars = ax2.bar(x_pos, percentiles, color=colors, alpha=0.7)\n",
    "ax2.axhline(y=50, color='gray', linestyle='--', alpha=0.7, label='Median')\n",
    "ax2.axhline(y=90, color='red', linestyle='--', alpha=0.7, label='90th Percentile')\n",
    "ax2.set_xlabel('Features')\n",
    "ax2.set_ylabel('Percentile')\n",
    "ax2.set_title('Percentiles of Optimal Values')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(feature_cols, rotation=45)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add labels with percentiles\n",
    "for bar, p in zip(bars, percentiles):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "            f'{p:.0f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 3. Comparison with best dataset samples\n",
    "ax3 = axes[1, 0]\n",
    "comparison_values = list(comparison_df['Prediction'][:-1]) + [optimal_prediction]\n",
    "labels = ['Top 1', 'Top 2', 'Top 3', 'Top 4', 'Top 5', 'Optimal']\n",
    "colors = ['lightblue'] * 5 + ['gold']\n",
    "\n",
    "bars = ax3.bar(labels, comparison_values, color=colors, alpha=0.7)\n",
    "ax3.set_xlabel('Samples')\n",
    "ax3.set_ylabel('Prediction')\n",
    "ax3.set_title('Comparison: Best Samples vs Optimal')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Add labels with values\n",
    "for bar, val in zip(bars, comparison_values):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(comparison_values)*0.01, \n",
    "            f'{val:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 4. Radar chart comparing optimal vs dataset average\n",
    "ax4 = axes[1, 1]\n",
    "angles = np.linspace(0, 2*np.pi, len(feature_cols), endpoint=False).tolist()\n",
    "angles += angles[:1]  # Complete the circle\n",
    "\n",
    "# Normalize values for radar chart\n",
    "scaler_radar = MinMaxScaler()\n",
    "data_for_radar = np.column_stack([optimal_weights, X[feature_cols].mean().values])\n",
    "normalized_data = scaler_radar.fit_transform(data_for_radar)\n",
    "\n",
    "optimal_normalized = normalized_data[:, 0].tolist()\n",
    "mean_normalized = normalized_data[:, 1].tolist()\n",
    "\n",
    "optimal_normalized += optimal_normalized[:1]\n",
    "mean_normalized += mean_normalized[:1]\n",
    "\n",
    "ax4 = plt.subplot(2, 2, 4, projection='polar')\n",
    "ax4.plot(angles, optimal_normalized, 'o-', linewidth=2, label='Optimal Values', color='gold')\n",
    "ax4.fill(angles, optimal_normalized, alpha=0.25, color='gold')\n",
    "ax4.plot(angles, mean_normalized, 'o-', linewidth=2, label='Dataset Average', color='blue')\n",
    "ax4.fill(angles, mean_normalized, alpha=0.25, color='blue')\n",
    "\n",
    "ax4.set_xticks(angles[:-1])\n",
    "ax4.set_xticklabels(feature_cols)\n",
    "ax4.set_ylim(0, 1)\n",
    "ax4.set_title('Radar Comparison: Optimal vs Average', y=1.08)\n",
    "ax4.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Final Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üéØ MACHINE LEARNING PROJECT FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä DATASET:\")\n",
    "print(f\"   ‚Ä¢ Size: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "print(f\"   ‚Ä¢ Features: {', '.join(feature_cols)}\")\n",
    "print(f\"   ‚Ä¢ Split: {X_train.shape[0]} train / {X_test.shape[0]} test\")\n",
    "\n",
    "print(f\"\\nü§ñ MODELS EVALUATED:\")\n",
    "models_tested = len(all_results)\n",
    "print(f\"   ‚Ä¢ Total models: {models_tested}\")\n",
    "print(f\"   ‚Ä¢ Linear Regression: {len(linear_results)} variants\")\n",
    "print(f\"   ‚Ä¢ Support Vector Regression: {len(svr_results)} kernels\")\n",
    "print(f\"   ‚Ä¢ XGBoost: 1 model with optimized hyperparameters\")\n",
    "\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
    "best_test_metrics = best_model_results['test_metrics']\n",
    "print(f\"   ‚Ä¢ R¬≤ Score: {best_test_metrics['R¬≤']:.4f}\")\n",
    "print(f\"   ‚Ä¢ RMSE: {best_test_metrics['RMSE']:.4f}\")\n",
    "print(f\"   ‚Ä¢ MAE: {best_test_metrics['MAE']:.4f}\")\n",
    "print(f\"   ‚Ä¢ MAPE: {best_test_metrics['MAPE']:.2f}%\")\n",
    "print(f\"   ‚Ä¢ CV RMSE: {best_model_results['cv_rmse']:.4f} (¬±{best_model_results['cv_rmse_std']:.4f})\")\n",
    "\n",
    "print(f\"\\nüéØ MOST IMPORTANT FEATURES:\")\n",
    "if best_model_name in importance_results:\n",
    "    top_features = importance_results[best_model_name].sort_values('Importance', ascending=False).head(3)\n",
    "    for i, (_, row) in enumerate(top_features.iterrows(), 1):\n",
    "        print(f\"   {i}. {row['Feature']}: {row['Importance']:.4f}\")\n",
    "\n",
    "print(f\"\\n‚ö° OPTIMAL WEIGHTS FOR MAXIMIZATION:\")\n",
    "print(f\"   ‚Ä¢ Maximum predicted value: {optimal_prediction:.4f}\")\n",
    "print(f\"   ‚Ä¢ Improvement over best sample: {improvement:.4f} ({improvement/max(top_5_predictions)*100:.2f}%)\")\n",
    "print(f\"   ‚Ä¢ Features to maximize:\")\n",
    "high_percentile_features = optimal_df[optimal_df['Percentile_in_Data'] > 75]\n",
    "for _, row in high_percentile_features.iterrows():\n",
    "    print(f\"     - {row['Feature']}: {row['Optimal_Value']:.2f} (percentile {row['Percentile_in_Data']:.0f})\")\n",
    "\n",
    "print(f\"\\nüìà MODEL QUALITY ANALYSIS:\")\n",
    "overfitting_score = best_model_results['train_metrics']['RMSE'] - best_model_results['test_metrics']['RMSE']\n",
    "if abs(overfitting_score) < 0.1 * best_model_results['test_metrics']['RMSE']:\n",
    "    print(f\"   ‚Ä¢ ‚úÖ Good balance between bias and variance\")\nelif overfitting_score > 0:\n",
    "    print(f\"   ‚Ä¢ ‚ö†Ô∏è Slight overfitting detected (difference: {overfitting_score:.4f})\")\nelse:\n",
    "    print(f\"   ‚Ä¢ ‚ö†Ô∏è Possible underfitting\")\n",
    "\n",
    "if best_test_metrics['R¬≤'] > 0.8:\n",
    "    quality = \"Excellent\"\nelif best_test_metrics['R¬≤'] > 0.6:\n",
    "    quality = \"Good\"\nelif best_test_metrics['R¬≤'] > 0.4:\n",
    "    quality = \"Fair\"\nelse:\n",
    "    quality = \"Needs improvement\"\n",
    "    \n",
    "print(f\"   ‚Ä¢ Fit quality: {quality} (R¬≤ = {best_test_metrics['R¬≤']:.4f})\")\n",
    "\n",
    "print(f\"\\nüî¨ RECOMMENDATIONS:\")\n",
    "print(f\"   ‚Ä¢ To maximize output, use the optimal weights found\")\n",
    "print(f\"   ‚Ä¢ Focus on the highest importance features identified\")\n",
    "\n",
    "if best_test_metrics['R¬≤'] < 0.8:\n",
    "    print(f\"   ‚Ä¢ Consider additional feature engineering to improve R¬≤\")\n",
    "    print(f\"   ‚Ä¢ Explore more complex models (Neural Networks, Ensemble methods)\")\n",
    "    \n",
    "if abs(overfitting_score) > 0.1 * best_model_results['test_metrics']['RMSE']:\n",
    "    print(f\"   ‚Ä¢ Consider additional regularization techniques\")\n",
    "    print(f\"   ‚Ä¢ Increase training dataset size if possible\")\n",
    "\n",
    "print(f\"   ‚Ä¢ Use cross-validation for production decisions\")\n",
    "print(f\"   ‚Ä¢ Monitor model performance on new data\")\n",
    "\n",
    "print(f\"\\nüíæ FILES GENERATED:\")\n",
    "print(f\"   ‚Ä¢ 01_exploratory_data_analysis_EN.ipynb: Complete exploratory analysis\")\n",
    "print(f\"   ‚Ä¢ 02_model_training_evaluation_EN.ipynb: Model training and evaluation\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚ú® PROJECT COMPLETED SUCCESSFULLY ‚ú®\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}